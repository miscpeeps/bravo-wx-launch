{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NeuralNetwork.ipynb\n",
    "\n",
    "Purpose: Create a neural network to model launch scrub/no scrub based on weather conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Gathering\n",
    "\n",
    "Data was produced from Bravo-Wx-Launch hackathon team. They created the raw-data-transform-multi.py script to compile of the data. I took all of this data and placed it into a single dataframe for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if pathlib.Path('combine.csv').exists():\n",
    "    df = pd.read_csv('combine.csv',index_col=0,header=0)\n",
    "else:\n",
    "    base_dir = pathlib.Path('/home/bearmint/projects/bravo-wx-launch/bravo-wx-launch/test-runs/test-run-20220722-0845 (complete)')\n",
    "    df_list = []\n",
    "    for filename in list(pathlib.Path.iterdir(base_dir)):\n",
    "        with open(pathlib.Path.joinpath(base_dir, filename), 'r') as f:\n",
    "            df_list.append(pd.read_csv(f))\n",
    "            \n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    df.set_index('Unnamed: 0', inplace=True)\n",
    "    df.to_csv('combine.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Balloon Wind Speed</th>\n",
       "      <th>Balloon Precipitable Water</th>\n",
       "      <th>Field Mill Mean</th>\n",
       "      <th>Sum of Lightning Strike Signals</th>\n",
       "      <th>Count of Lightning Strikes</th>\n",
       "      <th>Rain Gauge Inches</th>\n",
       "      <th>Avg Wind Speed 0002 NW  SE</th>\n",
       "      <th>Avg Wind Speed 0002 SE  SE</th>\n",
       "      <th>Avg Wind Speed 0006 NW  SE</th>\n",
       "      <th>Avg Wind Speed 0006 SE  SE</th>\n",
       "      <th>...</th>\n",
       "      <th>Peak Wind Speed 0006 NW  NW</th>\n",
       "      <th>Peak Wind Speed 0006 SE  NW</th>\n",
       "      <th>Deviation 0006 NW  NW</th>\n",
       "      <th>Deviation 0006 SE  NW</th>\n",
       "      <th>Temp 0006 NW  NW</th>\n",
       "      <th>Temp 0006 SE  NW</th>\n",
       "      <th>Temperature Difference 0006 NW  NW</th>\n",
       "      <th>Temperature Difference 0006 SE  NW</th>\n",
       "      <th>Barometric Pressure 0006 NW  NW</th>\n",
       "      <th>Barometric Pressure 0006 SE  NW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2015-12-21 21:29:00</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>140.290323</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-21 21:34:00</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>141.903226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-21 21:39:00</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>142.903226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-21 21:44:00</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>147.322581</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-12-21 21:49:00</th>\n",
       "      <td>20.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>147.129032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-14 02:24:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>118.225806</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-14 02:29:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>123.580645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-14 02:34:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>102.580645</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-14 02:39:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>75.193548</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015-03-14 02:44:00</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>55.064516</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6860 rows Ã— 175 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     Balloon Wind Speed  Balloon Precipitable Water  \\\n",
       "Unnamed: 0                                                            \n",
       "2015-12-21 21:29:00                20.9                         5.0   \n",
       "2015-12-21 21:34:00                20.9                         5.0   \n",
       "2015-12-21 21:39:00                20.9                         5.0   \n",
       "2015-12-21 21:44:00                20.9                         5.0   \n",
       "2015-12-21 21:49:00                20.9                         5.0   \n",
       "...                                 ...                         ...   \n",
       "2015-03-14 02:24:00                 NaN                         NaN   \n",
       "2015-03-14 02:29:00                 NaN                         NaN   \n",
       "2015-03-14 02:34:00                 NaN                         NaN   \n",
       "2015-03-14 02:39:00                 NaN                         NaN   \n",
       "2015-03-14 02:44:00                 NaN                         NaN   \n",
       "\n",
       "                     Field Mill Mean  Sum of Lightning Strike Signals  \\\n",
       "Unnamed: 0                                                              \n",
       "2015-12-21 21:29:00       140.290323                              0.0   \n",
       "2015-12-21 21:34:00       141.903226                              0.0   \n",
       "2015-12-21 21:39:00       142.903226                              0.0   \n",
       "2015-12-21 21:44:00       147.322581                              0.0   \n",
       "2015-12-21 21:49:00       147.129032                              0.0   \n",
       "...                              ...                              ...   \n",
       "2015-03-14 02:24:00       118.225806                              0.0   \n",
       "2015-03-14 02:29:00       123.580645                              0.0   \n",
       "2015-03-14 02:34:00       102.580645                              0.0   \n",
       "2015-03-14 02:39:00        75.193548                              0.0   \n",
       "2015-03-14 02:44:00        55.064516                              0.0   \n",
       "\n",
       "                     Count of Lightning Strikes  Rain Gauge Inches  \\\n",
       "Unnamed: 0                                                           \n",
       "2015-12-21 21:29:00                         0.0                  0   \n",
       "2015-12-21 21:34:00                         0.0                  0   \n",
       "2015-12-21 21:39:00                         0.0                  0   \n",
       "2015-12-21 21:44:00                         0.0                  0   \n",
       "2015-12-21 21:49:00                         0.0                  0   \n",
       "...                                         ...                ...   \n",
       "2015-03-14 02:24:00                         0.0                  0   \n",
       "2015-03-14 02:29:00                         0.0                  0   \n",
       "2015-03-14 02:34:00                         0.0                  0   \n",
       "2015-03-14 02:39:00                         0.0                  0   \n",
       "2015-03-14 02:44:00                         0.0                  0   \n",
       "\n",
       "                     Avg Wind Speed 0002 NW  SE  Avg Wind Speed 0002 SE  SE  \\\n",
       "Unnamed: 0                                                                    \n",
       "2015-12-21 21:29:00                         NaN                         NaN   \n",
       "2015-12-21 21:34:00                         NaN                         NaN   \n",
       "2015-12-21 21:39:00                         NaN                         NaN   \n",
       "2015-12-21 21:44:00                         NaN                         NaN   \n",
       "2015-12-21 21:49:00                         NaN                         NaN   \n",
       "...                                         ...                         ...   \n",
       "2015-03-14 02:24:00                         NaN                         NaN   \n",
       "2015-03-14 02:29:00                         NaN                         NaN   \n",
       "2015-03-14 02:34:00                         NaN                         NaN   \n",
       "2015-03-14 02:39:00                         NaN                         NaN   \n",
       "2015-03-14 02:44:00                         NaN                         NaN   \n",
       "\n",
       "                     Avg Wind Speed 0006 NW  SE  Avg Wind Speed 0006 SE  SE  \\\n",
       "Unnamed: 0                                                                    \n",
       "2015-12-21 21:29:00                         NaN                         NaN   \n",
       "2015-12-21 21:34:00                         NaN                         NaN   \n",
       "2015-12-21 21:39:00                         NaN                         NaN   \n",
       "2015-12-21 21:44:00                         NaN                         NaN   \n",
       "2015-12-21 21:49:00                         NaN                         NaN   \n",
       "...                                         ...                         ...   \n",
       "2015-03-14 02:24:00                         NaN                         NaN   \n",
       "2015-03-14 02:29:00                         NaN                         NaN   \n",
       "2015-03-14 02:34:00                         NaN                         NaN   \n",
       "2015-03-14 02:39:00                         NaN                         NaN   \n",
       "2015-03-14 02:44:00                         NaN                         NaN   \n",
       "\n",
       "                     ...  Peak Wind Speed 0006 NW  NW  \\\n",
       "Unnamed: 0           ...                                \n",
       "2015-12-21 21:29:00  ...                          NaN   \n",
       "2015-12-21 21:34:00  ...                          NaN   \n",
       "2015-12-21 21:39:00  ...                          NaN   \n",
       "2015-12-21 21:44:00  ...                          NaN   \n",
       "2015-12-21 21:49:00  ...                          NaN   \n",
       "...                  ...                          ...   \n",
       "2015-03-14 02:24:00  ...                          NaN   \n",
       "2015-03-14 02:29:00  ...                          NaN   \n",
       "2015-03-14 02:34:00  ...                          NaN   \n",
       "2015-03-14 02:39:00  ...                          NaN   \n",
       "2015-03-14 02:44:00  ...                          NaN   \n",
       "\n",
       "                     Peak Wind Speed 0006 SE  NW  Deviation 0006 NW  NW  \\\n",
       "Unnamed: 0                                                                \n",
       "2015-12-21 21:29:00                          NaN                    NaN   \n",
       "2015-12-21 21:34:00                          NaN                    NaN   \n",
       "2015-12-21 21:39:00                          NaN                    NaN   \n",
       "2015-12-21 21:44:00                          NaN                    NaN   \n",
       "2015-12-21 21:49:00                          NaN                    NaN   \n",
       "...                                          ...                    ...   \n",
       "2015-03-14 02:24:00                          NaN                    NaN   \n",
       "2015-03-14 02:29:00                          NaN                    NaN   \n",
       "2015-03-14 02:34:00                          NaN                    NaN   \n",
       "2015-03-14 02:39:00                          NaN                    NaN   \n",
       "2015-03-14 02:44:00                          NaN                    NaN   \n",
       "\n",
       "                     Deviation 0006 SE  NW  Temp 0006 NW  NW  \\\n",
       "Unnamed: 0                                                     \n",
       "2015-12-21 21:29:00                    NaN               NaN   \n",
       "2015-12-21 21:34:00                    NaN               NaN   \n",
       "2015-12-21 21:39:00                    NaN               NaN   \n",
       "2015-12-21 21:44:00                    NaN               NaN   \n",
       "2015-12-21 21:49:00                    NaN               NaN   \n",
       "...                                    ...               ...   \n",
       "2015-03-14 02:24:00                    NaN               NaN   \n",
       "2015-03-14 02:29:00                    NaN               NaN   \n",
       "2015-03-14 02:34:00                    NaN               NaN   \n",
       "2015-03-14 02:39:00                    NaN               NaN   \n",
       "2015-03-14 02:44:00                    NaN               NaN   \n",
       "\n",
       "                     Temp 0006 SE  NW  Temperature Difference 0006 NW  NW  \\\n",
       "Unnamed: 0                                                                  \n",
       "2015-12-21 21:29:00               NaN                                 NaN   \n",
       "2015-12-21 21:34:00               NaN                                 NaN   \n",
       "2015-12-21 21:39:00               NaN                                 NaN   \n",
       "2015-12-21 21:44:00               NaN                                 NaN   \n",
       "2015-12-21 21:49:00               NaN                                 NaN   \n",
       "...                               ...                                 ...   \n",
       "2015-03-14 02:24:00               NaN                                 NaN   \n",
       "2015-03-14 02:29:00               NaN                                 NaN   \n",
       "2015-03-14 02:34:00               NaN                                 NaN   \n",
       "2015-03-14 02:39:00               NaN                                 NaN   \n",
       "2015-03-14 02:44:00               NaN                                 NaN   \n",
       "\n",
       "                     Temperature Difference 0006 SE  NW  \\\n",
       "Unnamed: 0                                                \n",
       "2015-12-21 21:29:00                                 NaN   \n",
       "2015-12-21 21:34:00                                 NaN   \n",
       "2015-12-21 21:39:00                                 NaN   \n",
       "2015-12-21 21:44:00                                 NaN   \n",
       "2015-12-21 21:49:00                                 NaN   \n",
       "...                                                 ...   \n",
       "2015-03-14 02:24:00                                 NaN   \n",
       "2015-03-14 02:29:00                                 NaN   \n",
       "2015-03-14 02:34:00                                 NaN   \n",
       "2015-03-14 02:39:00                                 NaN   \n",
       "2015-03-14 02:44:00                                 NaN   \n",
       "\n",
       "                     Barometric Pressure 0006 NW  NW  \\\n",
       "Unnamed: 0                                             \n",
       "2015-12-21 21:29:00                              NaN   \n",
       "2015-12-21 21:34:00                              NaN   \n",
       "2015-12-21 21:39:00                              NaN   \n",
       "2015-12-21 21:44:00                              NaN   \n",
       "2015-12-21 21:49:00                              NaN   \n",
       "...                                              ...   \n",
       "2015-03-14 02:24:00                              NaN   \n",
       "2015-03-14 02:29:00                              NaN   \n",
       "2015-03-14 02:34:00                              NaN   \n",
       "2015-03-14 02:39:00                              NaN   \n",
       "2015-03-14 02:44:00                              NaN   \n",
       "\n",
       "                     Barometric Pressure 0006 SE  NW  \n",
       "Unnamed: 0                                            \n",
       "2015-12-21 21:29:00                              NaN  \n",
       "2015-12-21 21:34:00                              NaN  \n",
       "2015-12-21 21:39:00                              NaN  \n",
       "2015-12-21 21:44:00                              NaN  \n",
       "2015-12-21 21:49:00                              NaN  \n",
       "...                                              ...  \n",
       "2015-03-14 02:24:00                              NaN  \n",
       "2015-03-14 02:29:00                              NaN  \n",
       "2015-03-14 02:34:00                              NaN  \n",
       "2015-03-14 02:39:00                              NaN  \n",
       "2015-03-14 02:44:00                              NaN  \n",
       "\n",
       "[6860 rows x 175 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5488\n",
       "1    1372\n",
       "Name: scrub_id, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 200\n",
    "df.scrub_id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 6860 entries, 2015-12-21 21:29:00 to 2015-03-14 02:44:00\n",
      "Data columns (total 175 columns):\n",
      " #    Column                                       Non-Null Count  Dtype  \n",
      "---   ------                                       --------------  -----  \n",
      " 0    Balloon Wind Speed                           5978 non-null   float64\n",
      " 1    Balloon Precipitable Water                   5978 non-null   float64\n",
      " 2    Field Mill Mean                              6076 non-null   float64\n",
      " 3    Sum of Lightning Strike Signals              6860 non-null   float64\n",
      " 4    Count of Lightning Strikes                   6860 non-null   float64\n",
      " 5    Rain Gauge Inches                            6860 non-null   int64  \n",
      " 6    Avg Wind Speed 0002 NW  SE                   3577 non-null   float64\n",
      " 7    Avg Wind Speed 0002 SE  SE                   3577 non-null   float64\n",
      " 8    Avg Wind Speed 0006 NW  SE                   3381 non-null   float64\n",
      " 9    Avg Wind Speed 0006 SE  SE                   3381 non-null   float64\n",
      " 10   Avg Wind Speed 0110 NW  SE                   3234 non-null   float64\n",
      " 11   Avg Wind Speed 0110 SE  SE                   3234 non-null   float64\n",
      " 12   Avg Wind Speed 0313 NE  NE                   1078 non-null   float64\n",
      " 13   Avg Wind Speed 0313 SW  NE                   1078 non-null   float64\n",
      " 14   Avg Wind Speed SLC 40                        4508 non-null   float64\n",
      " 15   Avg Wind Speed SLC 41                        4508 non-null   float64\n",
      " 16   Peak Wind Speed 0002 NW  SE                  3577 non-null   float64\n",
      " 17   Peak Wind Speed 0002 SE  SE                  3577 non-null   float64\n",
      " 18   Peak Wind Speed 0006 NW  SE                  3381 non-null   float64\n",
      " 19   Peak Wind Speed 0006 SE  SE                  3381 non-null   float64\n",
      " 20   Peak Wind Speed 0110 NW  SE                  3234 non-null   float64\n",
      " 21   Peak Wind Speed 0110 SE  SE                  3234 non-null   float64\n",
      " 22   Peak Wind Speed 0313 NE  NE                  1078 non-null   float64\n",
      " 23   Peak Wind Speed 0313 SW  NE                  1078 non-null   float64\n",
      " 24   Peak Wind Speed SLC 40                       4508 non-null   float64\n",
      " 25   Peak Wind Speed SLC 41                       4508 non-null   float64\n",
      " 26   Deviation 0002 NW  SE                        3577 non-null   float64\n",
      " 27   Deviation 0002 SE  SE                        3577 non-null   float64\n",
      " 28   Deviation 0006 NW  SE                        3381 non-null   float64\n",
      " 29   Deviation 0006 SE  SE                        3381 non-null   float64\n",
      " 30   Deviation 0110 NW  SE                        3234 non-null   float64\n",
      " 31   Deviation 0110 SE  SE                        3234 non-null   float64\n",
      " 32   Deviation 0313 NE  NE                        1078 non-null   float64\n",
      " 33   Deviation 0313 SW  NE                        1078 non-null   float64\n",
      " 34   Deviation SLC 40                             4508 non-null   float64\n",
      " 35   Deviation SLC 41                             4508 non-null   float64\n",
      " 36   Temp 0002 NW  SE                             3577 non-null   float64\n",
      " 37   Temp 0002 SE  SE                             3577 non-null   float64\n",
      " 38   Temp 0006 NW  SE                             3381 non-null   float64\n",
      " 39   Temp 0006 SE  SE                             3381 non-null   float64\n",
      " 40   Temp 0110 NW  SE                             3234 non-null   float64\n",
      " 41   Temp 0110 SE  SE                             3234 non-null   float64\n",
      " 42   Temp 0313 NE  NE                             1078 non-null   float64\n",
      " 43   Temp 0313 SW  NE                             1078 non-null   float64\n",
      " 44   Temp SLC 40                                  4508 non-null   float64\n",
      " 45   Temp SLC 41                                  4508 non-null   float64\n",
      " 46   Temperature Difference 0002 NW  SE           3577 non-null   float64\n",
      " 47   Temperature Difference 0002 SE  SE           3577 non-null   float64\n",
      " 48   Temperature Difference 0006 NW  SE           3381 non-null   float64\n",
      " 49   Temperature Difference 0006 SE  SE           3381 non-null   float64\n",
      " 50   Temperature Difference 0110 NW  SE           3234 non-null   float64\n",
      " 51   Temperature Difference 0110 SE  SE           3234 non-null   float64\n",
      " 52   Temperature Difference 0313 NE  NE           1078 non-null   float64\n",
      " 53   Temperature Difference 0313 SW  NE           1078 non-null   float64\n",
      " 54   Temperature Difference SLC 40                4508 non-null   float64\n",
      " 55   Temperature Difference SLC 41                4508 non-null   float64\n",
      " 56   Barometric Pressure 0002 NW  SE              3577 non-null   float64\n",
      " 57   Barometric Pressure 0002 SE  SE              3577 non-null   float64\n",
      " 58   Barometric Pressure 0006 NW  SE              3381 non-null   float64\n",
      " 59   Barometric Pressure 0006 SE  SE              3381 non-null   float64\n",
      " 60   Barometric Pressure 0110 NW  SE              3234 non-null   float64\n",
      " 61   Barometric Pressure 0110 SE  SE              3234 non-null   float64\n",
      " 62   Barometric Pressure 0313 NE  NE              1078 non-null   float64\n",
      " 63   Barometric Pressure 0313 SW  NE              1078 non-null   float64\n",
      " 64   Barometric Pressure SLC 40                   4508 non-null   float64\n",
      " 65   Barometric Pressure SLC 41                   4508 non-null   float64\n",
      " 66   Altitude Height: 5000  m Speed (m/s)         6223 non-null   float64\n",
      " 67   Altitude Height: 5000  m Shear               6223 non-null   float64\n",
      " 68   Altitude Height: 5000  m WW?                 6223 non-null   float64\n",
      " 69   Altitude Height: 5000  m Direction (var)     6223 non-null   float64\n",
      " 70   Altitude Height: 8000  m Speed (m/s)         6223 non-null   float64\n",
      " 71   Altitude Height: 8000  m Shear               6223 non-null   float64\n",
      " 72   Altitude Height: 8000  m WW?                 6223 non-null   float64\n",
      " 73   Altitude Height: 8000  m Direction (var)     6223 non-null   float64\n",
      " 74   Altitude Height: 11000  m Speed (m/s)        6223 non-null   float64\n",
      " 75   Altitude Height: 11000  m Shear              6223 non-null   float64\n",
      " 76   Altitude Height: 11000  m WW?                6223 non-null   float64\n",
      " 77   Altitude Height: 11000  m Direction (var)    6223 non-null   float64\n",
      " 78   Altitude Height: 14000  m Speed (m/s)        6223 non-null   float64\n",
      " 79   Altitude Height: 14000  m Shear              6223 non-null   float64\n",
      " 80   Altitude Height: 14000  m WW?                6223 non-null   float64\n",
      " 81   Altitude Height: 14000  m Direction (var)    6223 non-null   float64\n",
      " 82   Altitude Height: 17000  m Speed (m/s)        6223 non-null   float64\n",
      " 83   Altitude Height: 17000  m Shear              6223 non-null   float64\n",
      " 84   Altitude Height: 17000  m WW?                6223 non-null   float64\n",
      " 85   Altitude Height: 17000  m Direction (var)    6223 non-null   float64\n",
      " 86   Altitude Height: 170000  m Speed (m/s)       6223 non-null   float64\n",
      " 87   Altitude Height: 170000  m Shear             6223 non-null   float64\n",
      " 88   Altitude Height: 170000  m WW?               6223 non-null   float64\n",
      " 89   Altitude Height: 170000  m Direction (var)   6223 non-null   float64\n",
      " 90   RWP0004 Max Height: 0.8  km Speed (m/s)      1372 non-null   float64\n",
      " 91   RWP0004 Max Height: 0.8  km Direction (var)  1372 non-null   float64\n",
      " 92   RWP0004 Max Height: 1.5  km Speed (m/s)      1372 non-null   float64\n",
      " 93   RWP0004 Max Height: 1.5  km Direction (var)  1372 non-null   float64\n",
      " 94   RWP0004 Max Height: 10  km Speed (m/s)       1372 non-null   float64\n",
      " 95   RWP0004 Max Height: 10  km Direction (var)   1372 non-null   float64\n",
      " 96   RWP0005 Max Height: 0.8  km Speed (m/s)      1323 non-null   float64\n",
      " 97   RWP0005 Max Height: 0.8  km Direction (var)  1323 non-null   float64\n",
      " 98   RWP0005 Max Height: 1.5  km Speed (m/s)      1323 non-null   float64\n",
      " 99   RWP0005 Max Height: 1.5  km Direction (var)  1323 non-null   float64\n",
      " 100  RWP0005 Max Height: 10  km Speed (m/s)       1323 non-null   float64\n",
      " 101  RWP0005 Max Height: 10  km Direction (var)   1323 non-null   float64\n",
      " 102  RWP0001 Max Height: 0.8  km Speed (m/s)      1274 non-null   float64\n",
      " 103  RWP0001 Max Height: 0.8  km Direction (var)  1274 non-null   float64\n",
      " 104  RWP0001 Max Height: 1.5  km Speed (m/s)      1274 non-null   float64\n",
      " 105  RWP0001 Max Height: 1.5  km Direction (var)  1274 non-null   float64\n",
      " 106  RWP0001 Max Height: 10  km Speed (m/s)       1274 non-null   float64\n",
      " 107  RWP0001 Max Height: 10  km Direction (var)   1274 non-null   float64\n",
      " 108  RWP0002 Max Height: 0.8  km Speed (m/s)      1225 non-null   float64\n",
      " 109  RWP0002 Max Height: 0.8  km Direction (var)  1225 non-null   float64\n",
      " 110  RWP0002 Max Height: 1.5  km Speed (m/s)      1323 non-null   float64\n",
      " 111  RWP0002 Max Height: 1.5  km Direction (var)  1323 non-null   float64\n",
      " 112  RWP0002 Max Height: 10  km Speed (m/s)       1323 non-null   float64\n",
      " 113  RWP0002 Max Height: 10  km Direction (var)   1323 non-null   float64\n",
      " 114  RWP0003 Max Height: 0.8  km Speed (m/s)      1372 non-null   float64\n",
      " 115  RWP0003 Max Height: 0.8  km Direction (var)  1372 non-null   float64\n",
      " 116  RWP0003 Max Height: 1.5  km Speed (m/s)      1372 non-null   float64\n",
      " 117  RWP0003 Max Height: 1.5  km Direction (var)  1372 non-null   float64\n",
      " 118  RWP0003 Max Height: 10  km Speed (m/s)       1372 non-null   float64\n",
      " 119  RWP0003 Max Height: 10  km Direction (var)   1372 non-null   float64\n",
      " 120  scrub_id                                     6860 non-null   int64  \n",
      " 121  Avg Wind Speed 0002 NW  NW                   2009 non-null   float64\n",
      " 122  Avg Wind Speed 0002 SE  NW                   2009 non-null   float64\n",
      " 123  Avg Wind Speed 0313 NE  SW                   3724 non-null   float64\n",
      " 124  Avg Wind Speed 0313 SW  SW                   3724 non-null   float64\n",
      " 125  Peak Wind Speed 0002 NW  NW                  2009 non-null   float64\n",
      " 126  Peak Wind Speed 0002 SE  NW                  2009 non-null   float64\n",
      " 127  Peak Wind Speed 0313 NE  SW                  3724 non-null   float64\n",
      " 128  Peak Wind Speed 0313 SW  SW                  3724 non-null   float64\n",
      " 129  Deviation 0002 NW  NW                        2009 non-null   float64\n",
      " 130  Deviation 0002 SE  NW                        2009 non-null   float64\n",
      " 131  Deviation 0313 NE  SW                        3724 non-null   float64\n",
      " 132  Deviation 0313 SW  SW                        3724 non-null   float64\n",
      " 133  Temp 0002 NW  NW                             2009 non-null   float64\n",
      " 134  Temp 0002 SE  NW                             2009 non-null   float64\n",
      " 135  Temp 0313 NE  SW                             3724 non-null   float64\n",
      " 136  Temp 0313 SW  SW                             3724 non-null   float64\n",
      " 137  Temperature Difference 0002 NW  NW           2009 non-null   float64\n",
      " 138  Temperature Difference 0002 SE  NW           2009 non-null   float64\n",
      " 139  Temperature Difference 0313 NE  SW           3724 non-null   float64\n",
      " 140  Temperature Difference 0313 SW  SW           3724 non-null   float64\n",
      " 141  Barometric Pressure 0002 NW  NW              2009 non-null   float64\n",
      " 142  Barometric Pressure 0002 SE  NW              2009 non-null   float64\n",
      " 143  Barometric Pressure 0313 NE  SW              3724 non-null   float64\n",
      " 144  Barometric Pressure 0313 SW  SW              3724 non-null   float64\n",
      " 145  Avg Wind Speed 0110 NW  NW                   2303 non-null   float64\n",
      " 146  Avg Wind Speed 0110 SE  NW                   2303 non-null   float64\n",
      " 147  Peak Wind Speed 0110 NW  NW                  2303 non-null   float64\n",
      " 148  Peak Wind Speed 0110 SE  NW                  2303 non-null   float64\n",
      " 149  Deviation 0110 NW  NW                        2303 non-null   float64\n",
      " 150  Deviation 0110 SE  NW                        2303 non-null   float64\n",
      " 151  Temp 0110 NW  NW                             2303 non-null   float64\n",
      " 152  Temp 0110 SE  NW                             2303 non-null   float64\n",
      " 153  Temperature Difference 0110 NW  NW           2303 non-null   float64\n",
      " 154  Temperature Difference 0110 SE  NW           2303 non-null   float64\n",
      " 155  Barometric Pressure 0110 NW  NW              2303 non-null   float64\n",
      " 156  Barometric Pressure 0110 SE  NW              2303 non-null   float64\n",
      " 157  Avg Wind Speed VAB 01                        1519 non-null   float64\n",
      " 158  Peak Wind Speed VAB 01                       1519 non-null   float64\n",
      " 159  Deviation VAB 01                             1519 non-null   float64\n",
      " 160  Temp VAB 01                                  1519 non-null   float64\n",
      " 161  Temperature Difference VAB 01                1519 non-null   float64\n",
      " 162  Barometric Pressure VAB 01                   1519 non-null   float64\n",
      " 163  Avg Wind Speed 0006 NW  NW                   2107 non-null   float64\n",
      " 164  Avg Wind Speed 0006 SE  NW                   2107 non-null   float64\n",
      " 165  Peak Wind Speed 0006 NW  NW                  2107 non-null   float64\n",
      " 166  Peak Wind Speed 0006 SE  NW                  2107 non-null   float64\n",
      " 167  Deviation 0006 NW  NW                        2107 non-null   float64\n",
      " 168  Deviation 0006 SE  NW                        2107 non-null   float64\n",
      " 169  Temp 0006 NW  NW                             2107 non-null   float64\n",
      " 170  Temp 0006 SE  NW                             2107 non-null   float64\n",
      " 171  Temperature Difference 0006 NW  NW           2107 non-null   float64\n",
      " 172  Temperature Difference 0006 SE  NW           2107 non-null   float64\n",
      " 173  Barometric Pressure 0006 NW  NW              2107 non-null   float64\n",
      " 174  Barometric Pressure 0006 SE  NW              2107 non-null   float64\n",
      "dtypes: float64(173), int64(2)\n",
      "memory usage: 9.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info(verbose=True, show_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "Quality issues\n",
    "- Columns with NaNs: Dropping columns with more than 1000 NaNs. Then imputing column values with a KNN imputer using the mean from the 5 nearest values\n",
    "  - Could potentially check each column to see if mean/mode/constant would be better to impute "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cols_to_remove = []\n",
    "#for column in df.columns:\n",
    "#    x = df[column].isna().sum()\n",
    "#    if x >= 4000:\n",
    "#        cols_to_remove.append(column)\n",
    "        #print(f'{column}: {x}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate targets\n",
    "target_df = df['scrub_id']\n",
    "df.drop(['scrub_id'], axis=1, inplace=True)\n",
    "#create df with few NaNs\n",
    "df_low_nan = df.copy()\n",
    "#find columns where NaNs are greater than 1000\n",
    "cols_to_remove = [column for column in df_low_nan.columns if df_low_nan[column].isna().sum()>=1000]\n",
    "df_low_nan.drop(cols_to_remove, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def make_data(df, target_df, imputer):\n",
    "    #splits data, imputes missing values, normalizes it and then returns data ready to be put in data laoders\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df, target_df,random_state=42)\n",
    "\n",
    "    pipe = Pipeline([('imputer', imputer), ('normalizer', Normalizer())])\n",
    "    train_df = pd.DataFrame(data=pipe.fit_transform(X_train),columns=df.columns)\n",
    "    train_data = []\n",
    "    for i in range(len(train_df.to_numpy())):\n",
    "        train_data.append([train_df.to_numpy()[i], y_train.to_numpy()[i]])\n",
    "\n",
    "    test_df = pd.DataFrame(data=pipe.fit_transform(X_test), columns=df.columns)\n",
    "    test_data = []\n",
    "    for i in range(len(test_df.to_numpy())):\n",
    "        test_data.append([test_df.to_numpy()[i], y_test.to_numpy()[i]])\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = make_data(df_low_nan, target_df, KNNImputer(n_neighbors=5))\n",
    "#train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loaders(train_data, test_data, batch_size, valid_size):\n",
    "    \"\"\"\n",
    "    Takes in training and test data and creates DataLoader objects\n",
    "    Inputs:\n",
    "        train_data: Transformed training data (must be torchvision.datasets.folder object (potentially only an ImageFolder))\n",
    "        test_data: Transformed testing data (must be torchvision.datasets.folder object (potentially only an ImageFolder))\n",
    "        batch_size: How many samples per batch (int)\n",
    "        valid_size: Percentage of training set to use for validation (float)\n",
    "    Outputs:\n",
    "        train_loader: Training data loader (DataLoader object)\n",
    "        valid_loader: Validation data loader (DataLoader object)\n",
    "        test_loader: Testing data loader (DataLoader object)\n",
    "    \"\"\"\n",
    "\n",
    "    #obtain indices for validation\n",
    "    num_train = len(train_data)\n",
    "    indices = list(range(num_train))\n",
    "    np.random.shuffle(indices)\n",
    "    split = int(np.floor(valid_size*num_train))\n",
    "    train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "    #samplers to select train/valid batches\n",
    "    train_sampler = torch.utils.data.sampler.SubsetRandomSampler(train_idx)\n",
    "    valid_sampler = torch.utils.data.sampler.SubsetRandomSampler(valid_idx)\n",
    "\n",
    "    #create data loaders by combining data and samplers\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=train_sampler)\n",
    "    valid_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, sampler=valid_sampler)\n",
    "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader\n",
    "\n",
    "train_loader, valid_loader, test_loader = create_loaders(train_data, test_data, batch_size=16, valid_size=0.2)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Used to check data format and that each input is an observation and its corresponding label - uncomment if you want to see the data\n",
    "#train_iter = iter(train_loader)\n",
    "#data, label = train_iter.next()\n",
    "#print(data.float(),label)\n",
    "#print(len(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize cuda "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on GPU\n"
     ]
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "if use_cuda:\n",
    "    print('Train on GPU')\n",
    "else:\n",
    "    print(\"Train on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (fc1): Linear(in_features=30, out_features=20, bias=True)\n",
       "  (fc2): Linear(in_features=20, out_features=10, bias=True)\n",
       "  (fc3): Linear(in_features=10, out_features=2, bias=True)\n",
       "  (dropout): Dropout(p=0.2, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(30,20)\n",
    "        self.fc2 = nn.Linear(20,10)\n",
    "        self.fc3 = nn.Linear(10,2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x\n",
    "model_low_nan = Network()\n",
    "model_low_nan\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function and Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "def get_optimizer(model):\n",
    "    return torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n_epochs, loaders, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \"\"\"returns trained model\"\"\"\n",
    "    # initialize tracker for minimum validation loss\n",
    "    valid_loss_min = np.Inf \n",
    "    \n",
    "    model.cuda()\n",
    "    \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        # initialize variables to monitor training and validation loss\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "        # set the module to training mode\n",
    "        model.train()\n",
    "        for data, target in loaders['train']:\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.float()\n",
    "            #clear gradients\n",
    "            optimizer.zero_grad()\n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            #calculate loss for batch\n",
    "            loss = criterion(output, target)\n",
    "            #backward pass\n",
    "            loss.backward()\n",
    "            #take optimization step\n",
    "            optimizer.step()\n",
    "            #update training loss\n",
    "            train_loss += loss.item()*data.size(0)    \n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        # set the model to evaluation mode\n",
    "        model.eval()\n",
    "        for batch_idx, (data, target) in enumerate(loaders['valid']):\n",
    "            # move to GPU\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            data = data.float()\n",
    "            #forward pass\n",
    "            output = model(data)\n",
    "            #calculate loss\n",
    "            loss = criterion(output, target)\n",
    "            #update validation loss\n",
    "            valid_loss += loss.item()*data.size(0)\n",
    "\n",
    "        train_loss = train_loss/len(loaders['train'].sampler)\n",
    "        valid_loss = valid_loss/len(loaders['valid'].sampler)\n",
    "\n",
    "        # print training/validation statistics \n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, \n",
    "            train_loss,\n",
    "            valid_loss\n",
    "            ))\n",
    "\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f'Validation loss decreased ({valid_loss_min:.6f} --> {valid_loss:.6f}. Saving model...')\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            valid_loss_min = valid_loss\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.542642 \tValidation Loss: 0.501396\n",
      "Validation loss decreased (inf --> 0.501396. Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.488750 \tValidation Loss: 0.490333\n",
      "Validation loss decreased (0.501396 --> 0.490333. Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.489475 \tValidation Loss: 0.484752\n",
      "Validation loss decreased (0.490333 --> 0.484752. Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.481310 \tValidation Loss: 0.478633\n",
      "Validation loss decreased (0.484752 --> 0.478633. Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.481232 \tValidation Loss: 0.476521\n",
      "Validation loss decreased (0.478633 --> 0.476521. Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.478903 \tValidation Loss: 0.475796\n",
      "Validation loss decreased (0.476521 --> 0.475796. Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.472853 \tValidation Loss: 0.471307\n",
      "Validation loss decreased (0.475796 --> 0.471307. Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.473271 \tValidation Loss: 0.469980\n",
      "Validation loss decreased (0.471307 --> 0.469980. Saving model...\n",
      "Epoch: 9 \tTraining Loss: 0.472431 \tValidation Loss: 0.468219\n",
      "Validation loss decreased (0.469980 --> 0.468219. Saving model...\n",
      "Epoch: 10 \tTraining Loss: 0.468716 \tValidation Loss: 0.466106\n",
      "Validation loss decreased (0.468219 --> 0.466106. Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.469594 \tValidation Loss: 0.466412\n",
      "Epoch: 12 \tTraining Loss: 0.468374 \tValidation Loss: 0.464966\n",
      "Validation loss decreased (0.466106 --> 0.464966. Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.466587 \tValidation Loss: 0.463525\n",
      "Validation loss decreased (0.464966 --> 0.463525. Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.470289 \tValidation Loss: 0.463311\n",
      "Validation loss decreased (0.463525 --> 0.463311. Saving model...\n",
      "Epoch: 15 \tTraining Loss: 0.467816 \tValidation Loss: 0.462987\n",
      "Validation loss decreased (0.463311 --> 0.462987. Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.465180 \tValidation Loss: 0.460924\n",
      "Validation loss decreased (0.462987 --> 0.460924. Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.468365 \tValidation Loss: 0.459956\n",
      "Validation loss decreased (0.460924 --> 0.459956. Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.461913 \tValidation Loss: 0.458673\n",
      "Validation loss decreased (0.459956 --> 0.458673. Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.462092 \tValidation Loss: 0.457836\n",
      "Validation loss decreased (0.458673 --> 0.457836. Saving model...\n",
      "Epoch: 20 \tTraining Loss: 0.465054 \tValidation Loss: 0.456286\n",
      "Validation loss decreased (0.457836 --> 0.456286. Saving model...\n",
      "Epoch: 21 \tTraining Loss: 0.461736 \tValidation Loss: 0.456607\n",
      "Epoch: 22 \tTraining Loss: 0.463505 \tValidation Loss: 0.454466\n",
      "Validation loss decreased (0.456286 --> 0.454466. Saving model...\n",
      "Epoch: 23 \tTraining Loss: 0.459358 \tValidation Loss: 0.452057\n",
      "Validation loss decreased (0.454466 --> 0.452057. Saving model...\n",
      "Epoch: 24 \tTraining Loss: 0.460351 \tValidation Loss: 0.452590\n",
      "Epoch: 25 \tTraining Loss: 0.456737 \tValidation Loss: 0.452279\n",
      "Epoch: 26 \tTraining Loss: 0.454911 \tValidation Loss: 0.449335\n",
      "Validation loss decreased (0.452057 --> 0.449335. Saving model...\n",
      "Epoch: 27 \tTraining Loss: 0.456452 \tValidation Loss: 0.448939\n",
      "Validation loss decreased (0.449335 --> 0.448939. Saving model...\n",
      "Epoch: 28 \tTraining Loss: 0.454680 \tValidation Loss: 0.448460\n",
      "Validation loss decreased (0.448939 --> 0.448460. Saving model...\n",
      "Epoch: 29 \tTraining Loss: 0.461901 \tValidation Loss: 0.448100\n",
      "Validation loss decreased (0.448460 --> 0.448100. Saving model...\n",
      "Epoch: 30 \tTraining Loss: 0.454172 \tValidation Loss: 0.449479\n",
      "Epoch: 31 \tTraining Loss: 0.454180 \tValidation Loss: 0.446074\n",
      "Validation loss decreased (0.448100 --> 0.446074. Saving model...\n",
      "Epoch: 32 \tTraining Loss: 0.448173 \tValidation Loss: 0.443046\n",
      "Validation loss decreased (0.446074 --> 0.443046. Saving model...\n",
      "Epoch: 33 \tTraining Loss: 0.453020 \tValidation Loss: 0.442540\n",
      "Validation loss decreased (0.443046 --> 0.442540. Saving model...\n",
      "Epoch: 34 \tTraining Loss: 0.454667 \tValidation Loss: 0.446099\n",
      "Epoch: 35 \tTraining Loss: 0.452196 \tValidation Loss: 0.442320\n",
      "Validation loss decreased (0.442540 --> 0.442320. Saving model...\n",
      "Epoch: 36 \tTraining Loss: 0.455749 \tValidation Loss: 0.444670\n",
      "Epoch: 37 \tTraining Loss: 0.456490 \tValidation Loss: 0.442101\n",
      "Validation loss decreased (0.442320 --> 0.442101. Saving model...\n",
      "Epoch: 38 \tTraining Loss: 0.453073 \tValidation Loss: 0.440481\n",
      "Validation loss decreased (0.442101 --> 0.440481. Saving model...\n",
      "Epoch: 39 \tTraining Loss: 0.453786 \tValidation Loss: 0.442366\n",
      "Epoch: 40 \tTraining Loss: 0.452410 \tValidation Loss: 0.438837\n",
      "Validation loss decreased (0.440481 --> 0.438837. Saving model...\n",
      "Epoch: 41 \tTraining Loss: 0.453032 \tValidation Loss: 0.440289\n",
      "Epoch: 42 \tTraining Loss: 0.449364 \tValidation Loss: 0.436359\n",
      "Validation loss decreased (0.438837 --> 0.436359. Saving model...\n",
      "Epoch: 43 \tTraining Loss: 0.449542 \tValidation Loss: 0.437018\n",
      "Epoch: 44 \tTraining Loss: 0.454332 \tValidation Loss: 0.434908\n",
      "Validation loss decreased (0.436359 --> 0.434908. Saving model...\n",
      "Epoch: 45 \tTraining Loss: 0.452892 \tValidation Loss: 0.437159\n",
      "Epoch: 46 \tTraining Loss: 0.451787 \tValidation Loss: 0.437165\n",
      "Epoch: 47 \tTraining Loss: 0.444031 \tValidation Loss: 0.434067\n",
      "Validation loss decreased (0.434908 --> 0.434067. Saving model...\n",
      "Epoch: 48 \tTraining Loss: 0.446148 \tValidation Loss: 0.432920\n",
      "Validation loss decreased (0.434067 --> 0.432920. Saving model...\n",
      "Epoch: 49 \tTraining Loss: 0.446537 \tValidation Loss: 0.433610\n",
      "Epoch: 50 \tTraining Loss: 0.444992 \tValidation Loss: 0.429919\n",
      "Validation loss decreased (0.432920 --> 0.429919. Saving model...\n",
      "Epoch: 51 \tTraining Loss: 0.448126 \tValidation Loss: 0.431773\n",
      "Epoch: 52 \tTraining Loss: 0.449291 \tValidation Loss: 0.430647\n",
      "Epoch: 53 \tTraining Loss: 0.446880 \tValidation Loss: 0.432253\n",
      "Epoch: 54 \tTraining Loss: 0.445656 \tValidation Loss: 0.433124\n",
      "Epoch: 55 \tTraining Loss: 0.448950 \tValidation Loss: 0.432233\n",
      "Epoch: 56 \tTraining Loss: 0.445113 \tValidation Loss: 0.433158\n",
      "Epoch: 57 \tTraining Loss: 0.445491 \tValidation Loss: 0.434412\n",
      "Epoch: 58 \tTraining Loss: 0.447333 \tValidation Loss: 0.430635\n",
      "Epoch: 59 \tTraining Loss: 0.436780 \tValidation Loss: 0.426959\n",
      "Validation loss decreased (0.429919 --> 0.426959. Saving model...\n",
      "Epoch: 60 \tTraining Loss: 0.446179 \tValidation Loss: 0.431081\n",
      "Epoch: 61 \tTraining Loss: 0.442918 \tValidation Loss: 0.426515\n",
      "Validation loss decreased (0.426959 --> 0.426515. Saving model...\n",
      "Epoch: 62 \tTraining Loss: 0.442630 \tValidation Loss: 0.433653\n",
      "Epoch: 63 \tTraining Loss: 0.441294 \tValidation Loss: 0.426519\n",
      "Epoch: 64 \tTraining Loss: 0.444084 \tValidation Loss: 0.422159\n",
      "Validation loss decreased (0.426515 --> 0.422159. Saving model...\n",
      "Epoch: 65 \tTraining Loss: 0.444656 \tValidation Loss: 0.430359\n",
      "Epoch: 66 \tTraining Loss: 0.447899 \tValidation Loss: 0.428840\n",
      "Epoch: 67 \tTraining Loss: 0.442762 \tValidation Loss: 0.422707\n",
      "Epoch: 68 \tTraining Loss: 0.439861 \tValidation Loss: 0.425397\n",
      "Epoch: 69 \tTraining Loss: 0.444993 \tValidation Loss: 0.423734\n",
      "Epoch: 70 \tTraining Loss: 0.439040 \tValidation Loss: 0.421401\n",
      "Validation loss decreased (0.422159 --> 0.421401. Saving model...\n",
      "Epoch: 71 \tTraining Loss: 0.442802 \tValidation Loss: 0.425663\n",
      "Epoch: 72 \tTraining Loss: 0.445706 \tValidation Loss: 0.424131\n",
      "Epoch: 73 \tTraining Loss: 0.441681 \tValidation Loss: 0.423033\n",
      "Epoch: 74 \tTraining Loss: 0.439955 \tValidation Loss: 0.420625\n",
      "Validation loss decreased (0.421401 --> 0.420625. Saving model...\n",
      "Epoch: 75 \tTraining Loss: 0.430867 \tValidation Loss: 0.426686\n",
      "Epoch: 76 \tTraining Loss: 0.443933 \tValidation Loss: 0.428835\n",
      "Epoch: 77 \tTraining Loss: 0.443721 \tValidation Loss: 0.423084\n",
      "Epoch: 78 \tTraining Loss: 0.435112 \tValidation Loss: 0.420689\n",
      "Epoch: 79 \tTraining Loss: 0.437380 \tValidation Loss: 0.421565\n",
      "Epoch: 80 \tTraining Loss: 0.439147 \tValidation Loss: 0.423502\n",
      "Epoch: 81 \tTraining Loss: 0.435649 \tValidation Loss: 0.422141\n",
      "Epoch: 82 \tTraining Loss: 0.434946 \tValidation Loss: 0.414954\n",
      "Validation loss decreased (0.420625 --> 0.414954. Saving model...\n",
      "Epoch: 83 \tTraining Loss: 0.433249 \tValidation Loss: 0.412316\n",
      "Validation loss decreased (0.414954 --> 0.412316. Saving model...\n",
      "Epoch: 84 \tTraining Loss: 0.436557 \tValidation Loss: 0.422492\n",
      "Epoch: 85 \tTraining Loss: 0.440795 \tValidation Loss: 0.420292\n",
      "Epoch: 86 \tTraining Loss: 0.445188 \tValidation Loss: 0.419983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 87 \tTraining Loss: 0.439959 \tValidation Loss: 0.418356\n",
      "Epoch: 88 \tTraining Loss: 0.439847 \tValidation Loss: 0.418458\n",
      "Epoch: 89 \tTraining Loss: 0.433792 \tValidation Loss: 0.422937\n",
      "Epoch: 90 \tTraining Loss: 0.439297 \tValidation Loss: 0.415852\n",
      "Epoch: 91 \tTraining Loss: 0.437502 \tValidation Loss: 0.419907\n",
      "Epoch: 92 \tTraining Loss: 0.443818 \tValidation Loss: 0.416989\n",
      "Epoch: 93 \tTraining Loss: 0.435739 \tValidation Loss: 0.417537\n",
      "Epoch: 94 \tTraining Loss: 0.431923 \tValidation Loss: 0.410680\n",
      "Validation loss decreased (0.412316 --> 0.410680. Saving model...\n",
      "Epoch: 95 \tTraining Loss: 0.428280 \tValidation Loss: 0.415722\n",
      "Epoch: 96 \tTraining Loss: 0.436084 \tValidation Loss: 0.421808\n",
      "Epoch: 97 \tTraining Loss: 0.433844 \tValidation Loss: 0.415830\n",
      "Epoch: 98 \tTraining Loss: 0.432905 \tValidation Loss: 0.413845\n",
      "Epoch: 99 \tTraining Loss: 0.434854 \tValidation Loss: 0.413732\n",
      "Epoch: 100 \tTraining Loss: 0.432415 \tValidation Loss: 0.412318\n",
      "Epoch: 101 \tTraining Loss: 0.433774 \tValidation Loss: 0.407142\n",
      "Validation loss decreased (0.410680 --> 0.407142. Saving model...\n",
      "Epoch: 102 \tTraining Loss: 0.436709 \tValidation Loss: 0.412704\n",
      "Epoch: 103 \tTraining Loss: 0.434069 \tValidation Loss: 0.419457\n",
      "Epoch: 104 \tTraining Loss: 0.432486 \tValidation Loss: 0.414535\n",
      "Epoch: 105 \tTraining Loss: 0.433208 \tValidation Loss: 0.416733\n",
      "Epoch: 106 \tTraining Loss: 0.432047 \tValidation Loss: 0.411162\n",
      "Epoch: 107 \tTraining Loss: 0.430922 \tValidation Loss: 0.413527\n",
      "Epoch: 108 \tTraining Loss: 0.436634 \tValidation Loss: 0.411466\n",
      "Epoch: 109 \tTraining Loss: 0.430368 \tValidation Loss: 0.411355\n",
      "Epoch: 110 \tTraining Loss: 0.429835 \tValidation Loss: 0.416913\n",
      "Epoch: 111 \tTraining Loss: 0.433228 \tValidation Loss: 0.412474\n",
      "Epoch: 112 \tTraining Loss: 0.427867 \tValidation Loss: 0.410945\n",
      "Epoch: 113 \tTraining Loss: 0.432557 \tValidation Loss: 0.415785\n",
      "Epoch: 114 \tTraining Loss: 0.435825 \tValidation Loss: 0.409850\n",
      "Epoch: 115 \tTraining Loss: 0.423472 \tValidation Loss: 0.412023\n",
      "Epoch: 116 \tTraining Loss: 0.430775 \tValidation Loss: 0.410704\n",
      "Epoch: 117 \tTraining Loss: 0.430943 \tValidation Loss: 0.414324\n",
      "Epoch: 118 \tTraining Loss: 0.430922 \tValidation Loss: 0.411751\n",
      "Epoch: 119 \tTraining Loss: 0.426331 \tValidation Loss: 0.413490\n",
      "Epoch: 120 \tTraining Loss: 0.429161 \tValidation Loss: 0.408210\n",
      "Epoch: 121 \tTraining Loss: 0.428687 \tValidation Loss: 0.412529\n",
      "Epoch: 122 \tTraining Loss: 0.429149 \tValidation Loss: 0.411173\n",
      "Epoch: 123 \tTraining Loss: 0.429904 \tValidation Loss: 0.407111\n",
      "Validation loss decreased (0.407142 --> 0.407111. Saving model...\n",
      "Epoch: 124 \tTraining Loss: 0.423250 \tValidation Loss: 0.409130\n",
      "Epoch: 125 \tTraining Loss: 0.424564 \tValidation Loss: 0.416486\n",
      "Epoch: 126 \tTraining Loss: 0.424089 \tValidation Loss: 0.415326\n",
      "Epoch: 127 \tTraining Loss: 0.424748 \tValidation Loss: 0.410842\n",
      "Epoch: 128 \tTraining Loss: 0.433008 \tValidation Loss: 0.419164\n",
      "Epoch: 129 \tTraining Loss: 0.432007 \tValidation Loss: 0.407637\n",
      "Epoch: 130 \tTraining Loss: 0.430782 \tValidation Loss: 0.412049\n",
      "Epoch: 131 \tTraining Loss: 0.428049 \tValidation Loss: 0.410444\n",
      "Epoch: 132 \tTraining Loss: 0.422954 \tValidation Loss: 0.408612\n",
      "Epoch: 133 \tTraining Loss: 0.425063 \tValidation Loss: 0.405280\n",
      "Validation loss decreased (0.407111 --> 0.405280. Saving model...\n",
      "Epoch: 134 \tTraining Loss: 0.432432 \tValidation Loss: 0.409902\n",
      "Epoch: 135 \tTraining Loss: 0.421961 \tValidation Loss: 0.404947\n",
      "Validation loss decreased (0.405280 --> 0.404947. Saving model...\n",
      "Epoch: 136 \tTraining Loss: 0.423259 \tValidation Loss: 0.409715\n",
      "Epoch: 137 \tTraining Loss: 0.428897 \tValidation Loss: 0.409956\n",
      "Epoch: 138 \tTraining Loss: 0.431279 \tValidation Loss: 0.425059\n",
      "Epoch: 139 \tTraining Loss: 0.425935 \tValidation Loss: 0.403633\n",
      "Validation loss decreased (0.404947 --> 0.403633. Saving model...\n",
      "Epoch: 140 \tTraining Loss: 0.421283 \tValidation Loss: 0.415896\n",
      "Epoch: 141 \tTraining Loss: 0.428735 \tValidation Loss: 0.417048\n",
      "Epoch: 142 \tTraining Loss: 0.424716 \tValidation Loss: 0.409972\n",
      "Epoch: 143 \tTraining Loss: 0.419290 \tValidation Loss: 0.414708\n",
      "Epoch: 144 \tTraining Loss: 0.422995 \tValidation Loss: 0.409799\n",
      "Epoch: 145 \tTraining Loss: 0.428493 \tValidation Loss: 0.415216\n",
      "Epoch: 146 \tTraining Loss: 0.425344 \tValidation Loss: 0.417486\n",
      "Epoch: 147 \tTraining Loss: 0.424565 \tValidation Loss: 0.412513\n",
      "Epoch: 148 \tTraining Loss: 0.416263 \tValidation Loss: 0.409677\n",
      "Epoch: 149 \tTraining Loss: 0.424164 \tValidation Loss: 0.407333\n",
      "Epoch: 150 \tTraining Loss: 0.425570 \tValidation Loss: 0.413439\n",
      "Epoch: 151 \tTraining Loss: 0.421588 \tValidation Loss: 0.412800\n",
      "Epoch: 152 \tTraining Loss: 0.429394 \tValidation Loss: 0.413761\n",
      "Epoch: 153 \tTraining Loss: 0.424113 \tValidation Loss: 0.421271\n",
      "Epoch: 154 \tTraining Loss: 0.426053 \tValidation Loss: 0.411600\n",
      "Epoch: 155 \tTraining Loss: 0.425843 \tValidation Loss: 0.406628\n",
      "Epoch: 156 \tTraining Loss: 0.417302 \tValidation Loss: 0.409686\n",
      "Epoch: 157 \tTraining Loss: 0.423528 \tValidation Loss: 0.408955\n",
      "Epoch: 158 \tTraining Loss: 0.423115 \tValidation Loss: 0.417498\n",
      "Epoch: 159 \tTraining Loss: 0.428134 \tValidation Loss: 0.411630\n",
      "Epoch: 160 \tTraining Loss: 0.420214 \tValidation Loss: 0.410378\n",
      "Epoch: 161 \tTraining Loss: 0.425562 \tValidation Loss: 0.406578\n",
      "Epoch: 162 \tTraining Loss: 0.426859 \tValidation Loss: 0.428911\n",
      "Epoch: 163 \tTraining Loss: 0.416935 \tValidation Loss: 0.420812\n",
      "Epoch: 164 \tTraining Loss: 0.419875 \tValidation Loss: 0.412001\n",
      "Epoch: 165 \tTraining Loss: 0.428075 \tValidation Loss: 0.415807\n",
      "Epoch: 166 \tTraining Loss: 0.421837 \tValidation Loss: 0.406629\n",
      "Epoch: 167 \tTraining Loss: 0.422355 \tValidation Loss: 0.415328\n",
      "Epoch: 168 \tTraining Loss: 0.420466 \tValidation Loss: 0.406163\n",
      "Epoch: 169 \tTraining Loss: 0.423534 \tValidation Loss: 0.403141\n",
      "Validation loss decreased (0.403633 --> 0.403141. Saving model...\n",
      "Epoch: 170 \tTraining Loss: 0.419370 \tValidation Loss: 0.413599\n",
      "Epoch: 171 \tTraining Loss: 0.426169 \tValidation Loss: 0.400154\n",
      "Validation loss decreased (0.403141 --> 0.400154. Saving model...\n",
      "Epoch: 172 \tTraining Loss: 0.418825 \tValidation Loss: 0.411774\n",
      "Epoch: 173 \tTraining Loss: 0.420974 \tValidation Loss: 0.405645\n",
      "Epoch: 174 \tTraining Loss: 0.425532 \tValidation Loss: 0.409207\n",
      "Epoch: 175 \tTraining Loss: 0.417259 \tValidation Loss: 0.406235\n",
      "Epoch: 176 \tTraining Loss: 0.418099 \tValidation Loss: 0.412237\n",
      "Epoch: 177 \tTraining Loss: 0.419239 \tValidation Loss: 0.400190\n",
      "Epoch: 178 \tTraining Loss: 0.418957 \tValidation Loss: 0.404786\n",
      "Epoch: 179 \tTraining Loss: 0.418118 \tValidation Loss: 0.404247\n",
      "Epoch: 180 \tTraining Loss: 0.414365 \tValidation Loss: 0.402893\n",
      "Epoch: 181 \tTraining Loss: 0.426846 \tValidation Loss: 0.416510\n",
      "Epoch: 182 \tTraining Loss: 0.424761 \tValidation Loss: 0.410215\n",
      "Epoch: 183 \tTraining Loss: 0.422045 \tValidation Loss: 0.399134\n",
      "Validation loss decreased (0.400154 --> 0.399134. Saving model...\n",
      "Epoch: 184 \tTraining Loss: 0.421466 \tValidation Loss: 0.404408\n",
      "Epoch: 185 \tTraining Loss: 0.424657 \tValidation Loss: 0.426030\n",
      "Epoch: 186 \tTraining Loss: 0.418176 \tValidation Loss: 0.408339\n",
      "Epoch: 187 \tTraining Loss: 0.419638 \tValidation Loss: 0.406844\n",
      "Epoch: 188 \tTraining Loss: 0.417063 \tValidation Loss: 0.399893\n",
      "Epoch: 189 \tTraining Loss: 0.418259 \tValidation Loss: 0.402308\n",
      "Epoch: 190 \tTraining Loss: 0.421331 \tValidation Loss: 0.404550\n",
      "Epoch: 191 \tTraining Loss: 0.416482 \tValidation Loss: 0.398831\n",
      "Validation loss decreased (0.399134 --> 0.398831. Saving model...\n",
      "Epoch: 192 \tTraining Loss: 0.423030 \tValidation Loss: 0.412448\n",
      "Epoch: 193 \tTraining Loss: 0.417152 \tValidation Loss: 0.404006\n",
      "Epoch: 194 \tTraining Loss: 0.418289 \tValidation Loss: 0.405263\n",
      "Epoch: 195 \tTraining Loss: 0.424823 \tValidation Loss: 0.404140\n",
      "Epoch: 196 \tTraining Loss: 0.413859 \tValidation Loss: 0.403274\n",
      "Epoch: 197 \tTraining Loss: 0.417506 \tValidation Loss: 0.403070\n",
      "Epoch: 198 \tTraining Loss: 0.416081 \tValidation Loss: 0.410036\n",
      "Epoch: 199 \tTraining Loss: 0.421151 \tValidation Loss: 0.414015\n",
      "Epoch: 200 \tTraining Loss: 0.426291 \tValidation Loss: 0.405453\n"
     ]
    }
   ],
   "source": [
    "model_low_nan = train(200, loaders, model_low_nan, get_optimizer(model_low_nan), criterion, use_cuda, 'model_low_nan.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.381627\n",
      "\n",
      "\n",
      "Test Accuracy: 84% (1448/1715)\n"
     ]
    }
   ],
   "source": [
    "def test(loaders, model, criterion, use_cuda):\n",
    "\n",
    "    # monitor test loss and accuracy\n",
    "    test_loss = 0.\n",
    "    correct = 0.\n",
    "    total = 0.\n",
    "\n",
    "    # set the module to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(loaders['test']):\n",
    "        # move to GPU\n",
    "        if use_cuda:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        data = data.float()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average test loss \n",
    "        test_loss = test_loss + ((1 / (batch_idx + 1)) * (loss.data.item() - test_loss))\n",
    "        # convert output probabilities to predicted class\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        # compare predictions to true label\n",
    "        correct += np.sum(np.squeeze(pred.eq(target.data.view_as(pred))).cpu().numpy())\n",
    "        total += data.size(0)\n",
    "            \n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "\n",
    "    print('\\nTest Accuracy: %2d%% (%2d/%2d)' % (\n",
    "        100. * correct / total, correct, total))\n",
    "\n",
    "# load the model that got the best validation accuracy\n",
    "model_low_nan.load_state_dict(torch.load('model_low_nan.pt'))\n",
    "test(loaders, model_low_nan, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with imputed NaNs\n",
    "\n",
    "Since there are so many missing NaNs three different imputing approaches will be taken\n",
    "1. Median Imputation\n",
    "2. KNN Imputation\n",
    "3. Iterative Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "train_data, test_data = make_data(df, target_df, SimpleImputer(strategy='median'))\n",
    "train_loader, valid_loader, test_loader = create_loaders(train_data, test_data, batch_size=16, valid_size=0.2)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(174,130)\n",
    "        self.fc2 = nn.Linear(130,90)\n",
    "        self.fc3 = nn.Linear(90,50)\n",
    "        self.fc4 = nn.Linear(50,2)\n",
    "\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.shape[0], -1)\n",
    "\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return x\n",
    "model_median = Network2()\n",
    "model_KNN = Network2()\n",
    "model_iter = Network2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.512313 \tValidation Loss: 0.490435\n",
      "Validation loss decreased (inf --> 0.490435. Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.504642 \tValidation Loss: 0.482513\n",
      "Validation loss decreased (0.490435 --> 0.482513. Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.494854 \tValidation Loss: 0.471092\n",
      "Validation loss decreased (0.482513 --> 0.471092. Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.472877 \tValidation Loss: 0.437304\n",
      "Validation loss decreased (0.471092 --> 0.437304. Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.456984 \tValidation Loss: 0.469665\n",
      "Epoch: 6 \tTraining Loss: 0.439307 \tValidation Loss: 0.451498\n",
      "Epoch: 7 \tTraining Loss: 0.433687 \tValidation Loss: 0.397753\n",
      "Validation loss decreased (0.437304 --> 0.397753. Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.431826 \tValidation Loss: 0.448947\n",
      "Epoch: 9 \tTraining Loss: 0.419631 \tValidation Loss: 0.503564\n",
      "Epoch: 10 \tTraining Loss: 0.417647 \tValidation Loss: 0.394131\n",
      "Validation loss decreased (0.397753 --> 0.394131. Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.419673 \tValidation Loss: 0.392957\n",
      "Validation loss decreased (0.394131 --> 0.392957. Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.409931 \tValidation Loss: 0.393386\n",
      "Epoch: 13 \tTraining Loss: 0.414787 \tValidation Loss: 0.386402\n",
      "Validation loss decreased (0.392957 --> 0.386402. Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.415397 \tValidation Loss: 0.395130\n",
      "Epoch: 15 \tTraining Loss: 0.412220 \tValidation Loss: 0.385594\n",
      "Validation loss decreased (0.386402 --> 0.385594. Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.400609 \tValidation Loss: 0.382397\n",
      "Validation loss decreased (0.385594 --> 0.382397. Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.406847 \tValidation Loss: 0.421332\n",
      "Epoch: 18 \tTraining Loss: 0.403062 \tValidation Loss: 0.390631\n",
      "Epoch: 19 \tTraining Loss: 0.398085 \tValidation Loss: 0.383915\n",
      "Epoch: 20 \tTraining Loss: 0.401439 \tValidation Loss: 0.379892\n",
      "Validation loss decreased (0.382397 --> 0.379892. Saving model...\n",
      "Epoch: 21 \tTraining Loss: 0.404210 \tValidation Loss: 0.391666\n",
      "Epoch: 22 \tTraining Loss: 0.396571 \tValidation Loss: 0.383074\n",
      "Epoch: 23 \tTraining Loss: 0.403274 \tValidation Loss: 0.380088\n",
      "Epoch: 24 \tTraining Loss: 0.396902 \tValidation Loss: 0.376360\n",
      "Validation loss decreased (0.379892 --> 0.376360. Saving model...\n",
      "Epoch: 25 \tTraining Loss: 0.401833 \tValidation Loss: 0.377087\n",
      "Epoch: 26 \tTraining Loss: 0.389437 \tValidation Loss: 0.382919\n",
      "Epoch: 27 \tTraining Loss: 0.395729 \tValidation Loss: 0.378126\n",
      "Epoch: 28 \tTraining Loss: 0.394358 \tValidation Loss: 0.372809\n",
      "Validation loss decreased (0.376360 --> 0.372809. Saving model...\n",
      "Epoch: 29 \tTraining Loss: 0.402570 \tValidation Loss: 0.386739\n",
      "Epoch: 30 \tTraining Loss: 0.391282 \tValidation Loss: 0.371065\n",
      "Validation loss decreased (0.372809 --> 0.371065. Saving model...\n",
      "Epoch: 31 \tTraining Loss: 0.399873 \tValidation Loss: 0.375214\n",
      "Epoch: 32 \tTraining Loss: 0.393714 \tValidation Loss: 0.387348\n",
      "Epoch: 33 \tTraining Loss: 0.391971 \tValidation Loss: 0.387877\n",
      "Epoch: 34 \tTraining Loss: 0.390322 \tValidation Loss: 0.389602\n",
      "Epoch: 35 \tTraining Loss: 0.388162 \tValidation Loss: 0.368467\n",
      "Validation loss decreased (0.371065 --> 0.368467. Saving model...\n",
      "Epoch: 36 \tTraining Loss: 0.391693 \tValidation Loss: 0.366783\n",
      "Validation loss decreased (0.368467 --> 0.366783. Saving model...\n",
      "Epoch: 37 \tTraining Loss: 0.387796 \tValidation Loss: 0.367506\n",
      "Epoch: 38 \tTraining Loss: 0.378195 \tValidation Loss: 0.371696\n",
      "Epoch: 39 \tTraining Loss: 0.387292 \tValidation Loss: 0.368492\n",
      "Epoch: 40 \tTraining Loss: 0.396538 \tValidation Loss: 0.409359\n",
      "Epoch: 41 \tTraining Loss: 0.390418 \tValidation Loss: 0.372710\n",
      "Epoch: 42 \tTraining Loss: 0.386600 \tValidation Loss: 0.370831\n",
      "Epoch: 43 \tTraining Loss: 0.384279 \tValidation Loss: 0.363222\n",
      "Validation loss decreased (0.366783 --> 0.363222. Saving model...\n",
      "Epoch: 44 \tTraining Loss: 0.382113 \tValidation Loss: 0.357989\n",
      "Validation loss decreased (0.363222 --> 0.357989. Saving model...\n",
      "Epoch: 45 \tTraining Loss: 0.376763 \tValidation Loss: 0.360879\n",
      "Epoch: 46 \tTraining Loss: 0.380411 \tValidation Loss: 0.358635\n",
      "Epoch: 47 \tTraining Loss: 0.375349 \tValidation Loss: 0.367137\n",
      "Epoch: 48 \tTraining Loss: 0.380679 \tValidation Loss: 0.377661\n",
      "Epoch: 49 \tTraining Loss: 0.383631 \tValidation Loss: 0.381043\n",
      "Epoch: 50 \tTraining Loss: 0.373617 \tValidation Loss: 0.354471\n",
      "Validation loss decreased (0.357989 --> 0.354471. Saving model...\n",
      "Epoch: 51 \tTraining Loss: 0.377112 \tValidation Loss: 0.371829\n",
      "Epoch: 52 \tTraining Loss: 0.373166 \tValidation Loss: 0.352210\n",
      "Validation loss decreased (0.354471 --> 0.352210. Saving model...\n",
      "Epoch: 53 \tTraining Loss: 0.382481 \tValidation Loss: 0.357602\n",
      "Epoch: 54 \tTraining Loss: 0.376424 \tValidation Loss: 0.357946\n",
      "Epoch: 55 \tTraining Loss: 0.371280 \tValidation Loss: 0.360455\n",
      "Epoch: 56 \tTraining Loss: 0.385264 \tValidation Loss: 0.348213\n",
      "Validation loss decreased (0.352210 --> 0.348213. Saving model...\n",
      "Epoch: 57 \tTraining Loss: 0.379424 \tValidation Loss: 0.349241\n",
      "Epoch: 58 \tTraining Loss: 0.372337 \tValidation Loss: 0.341347\n",
      "Validation loss decreased (0.348213 --> 0.341347. Saving model...\n",
      "Epoch: 59 \tTraining Loss: 0.375784 \tValidation Loss: 0.361304\n",
      "Epoch: 60 \tTraining Loss: 0.363705 \tValidation Loss: 0.348872\n",
      "Epoch: 61 \tTraining Loss: 0.374933 \tValidation Loss: 0.386637\n",
      "Epoch: 62 \tTraining Loss: 0.378387 \tValidation Loss: 0.365881\n",
      "Epoch: 63 \tTraining Loss: 0.366269 \tValidation Loss: 0.351816\n",
      "Epoch: 64 \tTraining Loss: 0.370689 \tValidation Loss: 0.334596\n",
      "Validation loss decreased (0.341347 --> 0.334596. Saving model...\n",
      "Epoch: 65 \tTraining Loss: 0.369795 \tValidation Loss: 0.369484\n",
      "Epoch: 66 \tTraining Loss: 0.367158 \tValidation Loss: 0.349397\n",
      "Epoch: 67 \tTraining Loss: 0.364224 \tValidation Loss: 0.328919\n",
      "Validation loss decreased (0.334596 --> 0.328919. Saving model...\n",
      "Epoch: 68 \tTraining Loss: 0.363783 \tValidation Loss: 0.387003\n",
      "Epoch: 69 \tTraining Loss: 0.370968 \tValidation Loss: 0.343404\n",
      "Epoch: 70 \tTraining Loss: 0.357661 \tValidation Loss: 0.336266\n",
      "Epoch: 71 \tTraining Loss: 0.366882 \tValidation Loss: 0.362048\n",
      "Epoch: 72 \tTraining Loss: 0.368649 \tValidation Loss: 0.331806\n",
      "Epoch: 73 \tTraining Loss: 0.366258 \tValidation Loss: 0.339449\n",
      "Epoch: 74 \tTraining Loss: 0.364087 \tValidation Loss: 0.332153\n",
      "Epoch: 75 \tTraining Loss: 0.365883 \tValidation Loss: 0.326513\n",
      "Validation loss decreased (0.328919 --> 0.326513. Saving model...\n",
      "Epoch: 76 \tTraining Loss: 0.366791 \tValidation Loss: 0.338484\n",
      "Epoch: 77 \tTraining Loss: 0.364191 \tValidation Loss: 0.342880\n",
      "Epoch: 78 \tTraining Loss: 0.362717 \tValidation Loss: 0.355610\n",
      "Epoch: 79 \tTraining Loss: 0.369641 \tValidation Loss: 0.330345\n",
      "Epoch: 80 \tTraining Loss: 0.369195 \tValidation Loss: 0.327110\n",
      "Epoch: 81 \tTraining Loss: 0.361184 \tValidation Loss: 0.336282\n",
      "Epoch: 82 \tTraining Loss: 0.358728 \tValidation Loss: 0.345641\n",
      "Epoch: 83 \tTraining Loss: 0.356897 \tValidation Loss: 0.332351\n",
      "Epoch: 84 \tTraining Loss: 0.358641 \tValidation Loss: 0.322021\n",
      "Validation loss decreased (0.326513 --> 0.322021. Saving model...\n",
      "Epoch: 85 \tTraining Loss: 0.359559 \tValidation Loss: 0.321822\n",
      "Validation loss decreased (0.322021 --> 0.321822. Saving model...\n",
      "Epoch: 86 \tTraining Loss: 0.356748 \tValidation Loss: 0.324473\n",
      "Epoch: 87 \tTraining Loss: 0.356625 \tValidation Loss: 0.321756\n",
      "Validation loss decreased (0.321822 --> 0.321756. Saving model...\n",
      "Epoch: 88 \tTraining Loss: 0.352855 \tValidation Loss: 0.357923\n",
      "Epoch: 89 \tTraining Loss: 0.353863 \tValidation Loss: 0.349466\n",
      "Epoch: 90 \tTraining Loss: 0.355410 \tValidation Loss: 0.332679\n",
      "Epoch: 91 \tTraining Loss: 0.360614 \tValidation Loss: 0.315253\n",
      "Validation loss decreased (0.321756 --> 0.315253. Saving model...\n",
      "Epoch: 92 \tTraining Loss: 0.354065 \tValidation Loss: 0.326420\n",
      "Epoch: 93 \tTraining Loss: 0.360948 \tValidation Loss: 0.332196\n",
      "Epoch: 94 \tTraining Loss: 0.364062 \tValidation Loss: 0.316793\n",
      "Epoch: 95 \tTraining Loss: 0.366574 \tValidation Loss: 0.336447\n",
      "Epoch: 96 \tTraining Loss: 0.357864 \tValidation Loss: 0.336529\n",
      "Epoch: 97 \tTraining Loss: 0.355089 \tValidation Loss: 0.337246\n",
      "Epoch: 98 \tTraining Loss: 0.349715 \tValidation Loss: 0.325038\n",
      "Epoch: 99 \tTraining Loss: 0.354016 \tValidation Loss: 0.328285\n",
      "Epoch: 100 \tTraining Loss: 0.359946 \tValidation Loss: 0.318636\n",
      "Epoch: 101 \tTraining Loss: 0.356172 \tValidation Loss: 0.327573\n",
      "Epoch: 102 \tTraining Loss: 0.357067 \tValidation Loss: 0.319724\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 103 \tTraining Loss: 0.352918 \tValidation Loss: 0.315456\n",
      "Epoch: 104 \tTraining Loss: 0.350202 \tValidation Loss: 0.314188\n",
      "Validation loss decreased (0.315253 --> 0.314188. Saving model...\n",
      "Epoch: 105 \tTraining Loss: 0.346669 \tValidation Loss: 0.332665\n",
      "Epoch: 106 \tTraining Loss: 0.361366 \tValidation Loss: 0.322518\n",
      "Epoch: 107 \tTraining Loss: 0.349904 \tValidation Loss: 0.334565\n",
      "Epoch: 108 \tTraining Loss: 0.365087 \tValidation Loss: 0.321393\n",
      "Epoch: 109 \tTraining Loss: 0.355435 \tValidation Loss: 0.318710\n",
      "Epoch: 110 \tTraining Loss: 0.362328 \tValidation Loss: 0.319847\n",
      "Epoch: 111 \tTraining Loss: 0.349640 \tValidation Loss: 0.323467\n",
      "Epoch: 112 \tTraining Loss: 0.352515 \tValidation Loss: 0.324956\n",
      "Epoch: 113 \tTraining Loss: 0.346944 \tValidation Loss: 0.321397\n",
      "Epoch: 114 \tTraining Loss: 0.347260 \tValidation Loss: 0.318327\n",
      "Epoch: 115 \tTraining Loss: 0.349696 \tValidation Loss: 0.320439\n",
      "Epoch: 116 \tTraining Loss: 0.352990 \tValidation Loss: 0.320238\n",
      "Epoch: 117 \tTraining Loss: 0.344201 \tValidation Loss: 0.314866\n",
      "Epoch: 118 \tTraining Loss: 0.346850 \tValidation Loss: 0.341994\n",
      "Epoch: 119 \tTraining Loss: 0.349059 \tValidation Loss: 0.319206\n",
      "Epoch: 120 \tTraining Loss: 0.349957 \tValidation Loss: 0.330724\n",
      "Epoch: 121 \tTraining Loss: 0.345547 \tValidation Loss: 0.327548\n",
      "Epoch: 122 \tTraining Loss: 0.342856 \tValidation Loss: 0.322507\n",
      "Epoch: 123 \tTraining Loss: 0.347077 \tValidation Loss: 0.309194\n",
      "Validation loss decreased (0.314188 --> 0.309194. Saving model...\n",
      "Epoch: 124 \tTraining Loss: 0.345028 \tValidation Loss: 0.310006\n",
      "Epoch: 125 \tTraining Loss: 0.342247 \tValidation Loss: 0.308224\n",
      "Validation loss decreased (0.309194 --> 0.308224. Saving model...\n",
      "Epoch: 126 \tTraining Loss: 0.342712 \tValidation Loss: 0.310156\n",
      "Epoch: 127 \tTraining Loss: 0.339448 \tValidation Loss: 0.317989\n",
      "Epoch: 128 \tTraining Loss: 0.350653 \tValidation Loss: 0.315888\n",
      "Epoch: 129 \tTraining Loss: 0.335719 \tValidation Loss: 0.331900\n",
      "Epoch: 130 \tTraining Loss: 0.338576 \tValidation Loss: 0.308935\n",
      "Epoch: 131 \tTraining Loss: 0.338130 \tValidation Loss: 0.308372\n",
      "Epoch: 132 \tTraining Loss: 0.354861 \tValidation Loss: 0.318675\n",
      "Epoch: 133 \tTraining Loss: 0.346756 \tValidation Loss: 0.300898\n",
      "Validation loss decreased (0.308224 --> 0.300898. Saving model...\n",
      "Epoch: 134 \tTraining Loss: 0.345969 \tValidation Loss: 0.363497\n",
      "Epoch: 135 \tTraining Loss: 0.345819 \tValidation Loss: 0.301729\n",
      "Epoch: 136 \tTraining Loss: 0.345947 \tValidation Loss: 0.302701\n",
      "Epoch: 137 \tTraining Loss: 0.343244 \tValidation Loss: 0.312051\n",
      "Epoch: 138 \tTraining Loss: 0.348527 \tValidation Loss: 0.320530\n",
      "Epoch: 139 \tTraining Loss: 0.345415 \tValidation Loss: 0.321205\n",
      "Epoch: 140 \tTraining Loss: 0.341825 \tValidation Loss: 0.311481\n",
      "Epoch: 141 \tTraining Loss: 0.339578 \tValidation Loss: 0.308688\n",
      "Epoch: 142 \tTraining Loss: 0.344604 \tValidation Loss: 0.307954\n",
      "Epoch: 143 \tTraining Loss: 0.342465 \tValidation Loss: 0.306274\n",
      "Epoch: 144 \tTraining Loss: 0.347382 \tValidation Loss: 0.331151\n",
      "Epoch: 145 \tTraining Loss: 0.346471 \tValidation Loss: 0.308317\n",
      "Epoch: 146 \tTraining Loss: 0.333153 \tValidation Loss: 0.289726\n",
      "Validation loss decreased (0.300898 --> 0.289726. Saving model...\n",
      "Epoch: 147 \tTraining Loss: 0.340423 \tValidation Loss: 0.327880\n",
      "Epoch: 148 \tTraining Loss: 0.339675 \tValidation Loss: 0.318374\n",
      "Epoch: 149 \tTraining Loss: 0.335825 \tValidation Loss: 0.320263\n",
      "Epoch: 150 \tTraining Loss: 0.337659 \tValidation Loss: 0.320784\n",
      "Epoch: 151 \tTraining Loss: 0.337459 \tValidation Loss: 0.307791\n",
      "Epoch: 152 \tTraining Loss: 0.338713 \tValidation Loss: 0.317177\n",
      "Epoch: 153 \tTraining Loss: 0.339837 \tValidation Loss: 0.309376\n",
      "Epoch: 154 \tTraining Loss: 0.341505 \tValidation Loss: 0.327330\n",
      "Epoch: 155 \tTraining Loss: 0.341435 \tValidation Loss: 0.310367\n",
      "Epoch: 156 \tTraining Loss: 0.327639 \tValidation Loss: 0.304178\n",
      "Epoch: 157 \tTraining Loss: 0.343143 \tValidation Loss: 0.312294\n",
      "Epoch: 158 \tTraining Loss: 0.333764 \tValidation Loss: 0.286830\n",
      "Validation loss decreased (0.289726 --> 0.286830. Saving model...\n",
      "Epoch: 159 \tTraining Loss: 0.346204 \tValidation Loss: 0.295562\n",
      "Epoch: 160 \tTraining Loss: 0.335588 \tValidation Loss: 0.295201\n",
      "Epoch: 161 \tTraining Loss: 0.341139 \tValidation Loss: 0.300483\n",
      "Epoch: 162 \tTraining Loss: 0.334631 \tValidation Loss: 0.286863\n",
      "Epoch: 163 \tTraining Loss: 0.338070 \tValidation Loss: 0.325884\n",
      "Epoch: 164 \tTraining Loss: 0.332239 \tValidation Loss: 0.293186\n",
      "Epoch: 165 \tTraining Loss: 0.333411 \tValidation Loss: 0.309483\n",
      "Epoch: 166 \tTraining Loss: 0.335729 \tValidation Loss: 0.288818\n",
      "Epoch: 167 \tTraining Loss: 0.335211 \tValidation Loss: 0.293099\n",
      "Epoch: 168 \tTraining Loss: 0.341009 \tValidation Loss: 0.289798\n",
      "Epoch: 169 \tTraining Loss: 0.335791 \tValidation Loss: 0.324823\n",
      "Epoch: 170 \tTraining Loss: 0.337762 \tValidation Loss: 0.310558\n",
      "Epoch: 171 \tTraining Loss: 0.326932 \tValidation Loss: 0.304329\n",
      "Epoch: 172 \tTraining Loss: 0.331017 \tValidation Loss: 0.304768\n",
      "Epoch: 173 \tTraining Loss: 0.332461 \tValidation Loss: 0.283038\n",
      "Validation loss decreased (0.286830 --> 0.283038. Saving model...\n",
      "Epoch: 174 \tTraining Loss: 0.329938 \tValidation Loss: 0.297956\n",
      "Epoch: 175 \tTraining Loss: 0.328686 \tValidation Loss: 0.290577\n",
      "Epoch: 176 \tTraining Loss: 0.324651 \tValidation Loss: 0.305827\n",
      "Epoch: 177 \tTraining Loss: 0.324938 \tValidation Loss: 0.283765\n",
      "Epoch: 178 \tTraining Loss: 0.336081 \tValidation Loss: 0.291460\n",
      "Epoch: 179 \tTraining Loss: 0.329262 \tValidation Loss: 0.303848\n",
      "Epoch: 180 \tTraining Loss: 0.333870 \tValidation Loss: 0.287662\n",
      "Epoch: 181 \tTraining Loss: 0.327644 \tValidation Loss: 0.299593\n",
      "Epoch: 182 \tTraining Loss: 0.328230 \tValidation Loss: 0.356220\n",
      "Epoch: 183 \tTraining Loss: 0.331520 \tValidation Loss: 0.290119\n",
      "Epoch: 184 \tTraining Loss: 0.324401 \tValidation Loss: 0.293978\n",
      "Epoch: 185 \tTraining Loss: 0.338714 \tValidation Loss: 0.287618\n",
      "Epoch: 186 \tTraining Loss: 0.327322 \tValidation Loss: 0.277150\n",
      "Validation loss decreased (0.283038 --> 0.277150. Saving model...\n",
      "Epoch: 187 \tTraining Loss: 0.321980 \tValidation Loss: 0.280602\n",
      "Epoch: 188 \tTraining Loss: 0.327382 \tValidation Loss: 0.280880\n",
      "Epoch: 189 \tTraining Loss: 0.336723 \tValidation Loss: 0.317117\n",
      "Epoch: 190 \tTraining Loss: 0.321098 \tValidation Loss: 0.273917\n",
      "Validation loss decreased (0.277150 --> 0.273917. Saving model...\n",
      "Epoch: 191 \tTraining Loss: 0.335515 \tValidation Loss: 0.302952\n",
      "Epoch: 192 \tTraining Loss: 0.322388 \tValidation Loss: 0.273568\n",
      "Validation loss decreased (0.273917 --> 0.273568. Saving model...\n",
      "Epoch: 193 \tTraining Loss: 0.322311 \tValidation Loss: 0.297740\n",
      "Epoch: 194 \tTraining Loss: 0.322642 \tValidation Loss: 0.316841\n",
      "Epoch: 195 \tTraining Loss: 0.320719 \tValidation Loss: 0.288054\n",
      "Epoch: 196 \tTraining Loss: 0.329779 \tValidation Loss: 0.280676\n",
      "Epoch: 197 \tTraining Loss: 0.331723 \tValidation Loss: 0.296457\n",
      "Epoch: 198 \tTraining Loss: 0.324097 \tValidation Loss: 0.300438\n",
      "Epoch: 199 \tTraining Loss: 0.330349 \tValidation Loss: 0.278179\n",
      "Epoch: 200 \tTraining Loss: 0.316440 \tValidation Loss: 0.274728\n"
     ]
    }
   ],
   "source": [
    "model_median = train(200, loaders, model_median, get_optimizer(model_median), criterion, use_cuda, 'model_median.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.257980\n",
      "\n",
      "\n",
      "Test Accuracy: 90% (1544/1715)\n"
     ]
    }
   ],
   "source": [
    "model_median.load_state_dict(torch.load('model_median.pt'))\n",
    "test(loaders, model_median, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.516918 \tValidation Loss: 0.509519\n",
      "Validation loss decreased (inf --> 0.509519. Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.496750 \tValidation Loss: 0.495660\n",
      "Validation loss decreased (0.509519 --> 0.495660. Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.484602 \tValidation Loss: 0.477047\n",
      "Validation loss decreased (0.495660 --> 0.477047. Saving model...\n",
      "Epoch: 4 \tTraining Loss: 0.461894 \tValidation Loss: 0.490037\n",
      "Epoch: 5 \tTraining Loss: 0.443528 \tValidation Loss: 0.414561\n",
      "Validation loss decreased (0.477047 --> 0.414561. Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.439593 \tValidation Loss: 0.413834\n",
      "Validation loss decreased (0.414561 --> 0.413834. Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.425073 \tValidation Loss: 0.406548\n",
      "Validation loss decreased (0.413834 --> 0.406548. Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.418819 \tValidation Loss: 0.412533\n",
      "Epoch: 9 \tTraining Loss: 0.419371 \tValidation Loss: 0.422283\n",
      "Epoch: 10 \tTraining Loss: 0.422314 \tValidation Loss: 0.399777\n",
      "Validation loss decreased (0.406548 --> 0.399777. Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.418016 \tValidation Loss: 0.421601\n",
      "Epoch: 12 \tTraining Loss: 0.416596 \tValidation Loss: 0.399748\n",
      "Validation loss decreased (0.399777 --> 0.399748. Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.415338 \tValidation Loss: 0.421431\n",
      "Epoch: 14 \tTraining Loss: 0.409041 \tValidation Loss: 0.448571\n",
      "Epoch: 15 \tTraining Loss: 0.409649 \tValidation Loss: 0.395308\n",
      "Validation loss decreased (0.399748 --> 0.395308. Saving model...\n",
      "Epoch: 16 \tTraining Loss: 0.402839 \tValidation Loss: 0.392193\n",
      "Validation loss decreased (0.395308 --> 0.392193. Saving model...\n",
      "Epoch: 17 \tTraining Loss: 0.405111 \tValidation Loss: 0.397477\n",
      "Epoch: 18 \tTraining Loss: 0.406344 \tValidation Loss: 0.386285\n",
      "Validation loss decreased (0.392193 --> 0.386285. Saving model...\n",
      "Epoch: 19 \tTraining Loss: 0.407179 \tValidation Loss: 0.402395\n",
      "Epoch: 20 \tTraining Loss: 0.399882 \tValidation Loss: 0.414646\n",
      "Epoch: 21 \tTraining Loss: 0.398600 \tValidation Loss: 0.401180\n",
      "Epoch: 22 \tTraining Loss: 0.399876 \tValidation Loss: 0.377069\n",
      "Validation loss decreased (0.386285 --> 0.377069. Saving model...\n",
      "Epoch: 23 \tTraining Loss: 0.399088 \tValidation Loss: 0.374986\n",
      "Validation loss decreased (0.377069 --> 0.374986. Saving model...\n",
      "Epoch: 24 \tTraining Loss: 0.391371 \tValidation Loss: 0.383134\n",
      "Epoch: 25 \tTraining Loss: 0.394189 \tValidation Loss: 0.373719\n",
      "Validation loss decreased (0.374986 --> 0.373719. Saving model...\n",
      "Epoch: 26 \tTraining Loss: 0.400227 \tValidation Loss: 0.389904\n",
      "Epoch: 27 \tTraining Loss: 0.393098 \tValidation Loss: 0.368248\n",
      "Validation loss decreased (0.373719 --> 0.368248. Saving model...\n",
      "Epoch: 28 \tTraining Loss: 0.387371 \tValidation Loss: 0.376532\n",
      "Epoch: 29 \tTraining Loss: 0.383411 \tValidation Loss: 0.386625\n",
      "Epoch: 30 \tTraining Loss: 0.397100 \tValidation Loss: 0.370519\n",
      "Epoch: 31 \tTraining Loss: 0.395822 \tValidation Loss: 0.383889\n",
      "Epoch: 32 \tTraining Loss: 0.385883 \tValidation Loss: 0.365334\n",
      "Validation loss decreased (0.368248 --> 0.365334. Saving model...\n",
      "Epoch: 33 \tTraining Loss: 0.386551 \tValidation Loss: 0.371096\n",
      "Epoch: 34 \tTraining Loss: 0.377651 \tValidation Loss: 0.356087\n",
      "Validation loss decreased (0.365334 --> 0.356087. Saving model...\n",
      "Epoch: 35 \tTraining Loss: 0.379166 \tValidation Loss: 0.352861\n",
      "Validation loss decreased (0.356087 --> 0.352861. Saving model...\n",
      "Epoch: 36 \tTraining Loss: 0.389400 \tValidation Loss: 0.365793\n",
      "Epoch: 37 \tTraining Loss: 0.379402 \tValidation Loss: 0.382977\n",
      "Epoch: 38 \tTraining Loss: 0.379330 \tValidation Loss: 0.364501\n",
      "Epoch: 39 \tTraining Loss: 0.376748 \tValidation Loss: 0.338846\n",
      "Validation loss decreased (0.352861 --> 0.338846. Saving model...\n",
      "Epoch: 40 \tTraining Loss: 0.381874 \tValidation Loss: 0.375151\n",
      "Epoch: 41 \tTraining Loss: 0.376063 \tValidation Loss: 0.343668\n",
      "Epoch: 42 \tTraining Loss: 0.371995 \tValidation Loss: 0.359313\n",
      "Epoch: 43 \tTraining Loss: 0.366145 \tValidation Loss: 0.370390\n",
      "Epoch: 44 \tTraining Loss: 0.365558 \tValidation Loss: 0.376892\n",
      "Epoch: 45 \tTraining Loss: 0.369954 \tValidation Loss: 0.344631\n",
      "Epoch: 46 \tTraining Loss: 0.366363 \tValidation Loss: 0.362735\n",
      "Epoch: 47 \tTraining Loss: 0.373023 \tValidation Loss: 0.340131\n",
      "Epoch: 48 \tTraining Loss: 0.363151 \tValidation Loss: 0.327178\n",
      "Validation loss decreased (0.338846 --> 0.327178. Saving model...\n",
      "Epoch: 49 \tTraining Loss: 0.366077 \tValidation Loss: 0.324097\n",
      "Validation loss decreased (0.327178 --> 0.324097. Saving model...\n",
      "Epoch: 50 \tTraining Loss: 0.365969 \tValidation Loss: 0.343961\n",
      "Epoch: 51 \tTraining Loss: 0.354785 \tValidation Loss: 0.327048\n",
      "Epoch: 52 \tTraining Loss: 0.366574 \tValidation Loss: 0.339831\n",
      "Epoch: 53 \tTraining Loss: 0.359570 \tValidation Loss: 0.325094\n",
      "Epoch: 54 \tTraining Loss: 0.360847 \tValidation Loss: 0.320886\n",
      "Validation loss decreased (0.324097 --> 0.320886. Saving model...\n",
      "Epoch: 55 \tTraining Loss: 0.351460 \tValidation Loss: 0.319580\n",
      "Validation loss decreased (0.320886 --> 0.319580. Saving model...\n",
      "Epoch: 56 \tTraining Loss: 0.354039 \tValidation Loss: 0.327778\n",
      "Epoch: 57 \tTraining Loss: 0.354013 \tValidation Loss: 0.371959\n",
      "Epoch: 58 \tTraining Loss: 0.352116 \tValidation Loss: 0.310183\n",
      "Validation loss decreased (0.319580 --> 0.310183. Saving model...\n",
      "Epoch: 59 \tTraining Loss: 0.356762 \tValidation Loss: 0.335265\n",
      "Epoch: 60 \tTraining Loss: 0.359131 \tValidation Loss: 0.318864\n",
      "Epoch: 61 \tTraining Loss: 0.350405 \tValidation Loss: 0.315222\n",
      "Epoch: 62 \tTraining Loss: 0.350066 \tValidation Loss: 0.348300\n",
      "Epoch: 63 \tTraining Loss: 0.351655 \tValidation Loss: 0.348162\n",
      "Epoch: 64 \tTraining Loss: 0.343707 \tValidation Loss: 0.305671\n",
      "Validation loss decreased (0.310183 --> 0.305671. Saving model...\n",
      "Epoch: 65 \tTraining Loss: 0.350171 \tValidation Loss: 0.325686\n",
      "Epoch: 66 \tTraining Loss: 0.336980 \tValidation Loss: 0.305939\n",
      "Epoch: 67 \tTraining Loss: 0.349919 \tValidation Loss: 0.306049\n",
      "Epoch: 68 \tTraining Loss: 0.337199 \tValidation Loss: 0.309561\n",
      "Epoch: 69 \tTraining Loss: 0.355566 \tValidation Loss: 0.303346\n",
      "Validation loss decreased (0.305671 --> 0.303346. Saving model...\n",
      "Epoch: 70 \tTraining Loss: 0.344279 \tValidation Loss: 0.311670\n",
      "Epoch: 71 \tTraining Loss: 0.342120 \tValidation Loss: 0.304646\n",
      "Epoch: 72 \tTraining Loss: 0.337986 \tValidation Loss: 0.304814\n",
      "Epoch: 73 \tTraining Loss: 0.347310 \tValidation Loss: 0.296289\n",
      "Validation loss decreased (0.303346 --> 0.296289. Saving model...\n",
      "Epoch: 74 \tTraining Loss: 0.331891 \tValidation Loss: 0.290010\n",
      "Validation loss decreased (0.296289 --> 0.290010. Saving model...\n",
      "Epoch: 75 \tTraining Loss: 0.335784 \tValidation Loss: 0.301682\n",
      "Epoch: 76 \tTraining Loss: 0.343404 \tValidation Loss: 0.293825\n",
      "Epoch: 77 \tTraining Loss: 0.342689 \tValidation Loss: 0.309411\n",
      "Epoch: 78 \tTraining Loss: 0.340799 \tValidation Loss: 0.304921\n",
      "Epoch: 79 \tTraining Loss: 0.340618 \tValidation Loss: 0.322667\n",
      "Epoch: 80 \tTraining Loss: 0.330236 \tValidation Loss: 0.289464\n",
      "Validation loss decreased (0.290010 --> 0.289464. Saving model...\n",
      "Epoch: 81 \tTraining Loss: 0.343140 \tValidation Loss: 0.289795\n",
      "Epoch: 82 \tTraining Loss: 0.343803 \tValidation Loss: 0.310351\n",
      "Epoch: 83 \tTraining Loss: 0.330400 \tValidation Loss: 0.293335\n",
      "Epoch: 84 \tTraining Loss: 0.334571 \tValidation Loss: 0.301040\n",
      "Epoch: 85 \tTraining Loss: 0.334011 \tValidation Loss: 0.331313\n",
      "Epoch: 86 \tTraining Loss: 0.334789 \tValidation Loss: 0.291353\n",
      "Epoch: 87 \tTraining Loss: 0.331014 \tValidation Loss: 0.345768\n",
      "Epoch: 88 \tTraining Loss: 0.331251 \tValidation Loss: 0.296542\n",
      "Epoch: 89 \tTraining Loss: 0.327488 \tValidation Loss: 0.293649\n",
      "Epoch: 90 \tTraining Loss: 0.335556 \tValidation Loss: 0.304868\n",
      "Epoch: 91 \tTraining Loss: 0.324275 \tValidation Loss: 0.307659\n",
      "Epoch: 92 \tTraining Loss: 0.328032 \tValidation Loss: 0.402640\n",
      "Epoch: 93 \tTraining Loss: 0.335993 \tValidation Loss: 0.291726\n",
      "Epoch: 94 \tTraining Loss: 0.327349 \tValidation Loss: 0.294648\n",
      "Epoch: 95 \tTraining Loss: 0.331309 \tValidation Loss: 0.321306\n",
      "Epoch: 96 \tTraining Loss: 0.333995 \tValidation Loss: 0.282688\n",
      "Validation loss decreased (0.289464 --> 0.282688. Saving model...\n",
      "Epoch: 97 \tTraining Loss: 0.332371 \tValidation Loss: 0.339995\n",
      "Epoch: 98 \tTraining Loss: 0.328566 \tValidation Loss: 0.293053\n",
      "Epoch: 99 \tTraining Loss: 0.326826 \tValidation Loss: 0.284903\n",
      "Epoch: 100 \tTraining Loss: 0.330060 \tValidation Loss: 0.307378\n",
      "Epoch: 101 \tTraining Loss: 0.329416 \tValidation Loss: 0.298102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 102 \tTraining Loss: 0.337270 \tValidation Loss: 0.296873\n",
      "Epoch: 103 \tTraining Loss: 0.326066 \tValidation Loss: 0.280684\n",
      "Validation loss decreased (0.282688 --> 0.280684. Saving model...\n",
      "Epoch: 104 \tTraining Loss: 0.326790 \tValidation Loss: 0.277931\n",
      "Validation loss decreased (0.280684 --> 0.277931. Saving model...\n",
      "Epoch: 105 \tTraining Loss: 0.315144 \tValidation Loss: 0.264161\n",
      "Validation loss decreased (0.277931 --> 0.264161. Saving model...\n",
      "Epoch: 106 \tTraining Loss: 0.333078 \tValidation Loss: 0.336608\n",
      "Epoch: 107 \tTraining Loss: 0.330195 \tValidation Loss: 0.276710\n",
      "Epoch: 108 \tTraining Loss: 0.327115 \tValidation Loss: 0.288778\n",
      "Epoch: 109 \tTraining Loss: 0.323421 \tValidation Loss: 0.306237\n",
      "Epoch: 110 \tTraining Loss: 0.324067 \tValidation Loss: 0.302803\n",
      "Epoch: 111 \tTraining Loss: 0.325541 \tValidation Loss: 0.284047\n",
      "Epoch: 112 \tTraining Loss: 0.322377 \tValidation Loss: 0.275549\n",
      "Epoch: 113 \tTraining Loss: 0.317272 \tValidation Loss: 0.291535\n",
      "Epoch: 114 \tTraining Loss: 0.332539 \tValidation Loss: 0.282673\n",
      "Epoch: 115 \tTraining Loss: 0.327050 \tValidation Loss: 0.289549\n",
      "Epoch: 116 \tTraining Loss: 0.328788 \tValidation Loss: 0.288925\n",
      "Epoch: 117 \tTraining Loss: 0.328253 \tValidation Loss: 0.290082\n",
      "Epoch: 118 \tTraining Loss: 0.329505 \tValidation Loss: 0.295838\n",
      "Epoch: 119 \tTraining Loss: 0.320889 \tValidation Loss: 0.284278\n",
      "Epoch: 120 \tTraining Loss: 0.312290 \tValidation Loss: 0.257938\n",
      "Validation loss decreased (0.264161 --> 0.257938. Saving model...\n",
      "Epoch: 121 \tTraining Loss: 0.325966 \tValidation Loss: 0.259475\n",
      "Epoch: 122 \tTraining Loss: 0.312027 \tValidation Loss: 0.253311\n",
      "Validation loss decreased (0.257938 --> 0.253311. Saving model...\n",
      "Epoch: 123 \tTraining Loss: 0.324134 \tValidation Loss: 0.271521\n",
      "Epoch: 124 \tTraining Loss: 0.321100 \tValidation Loss: 0.277451\n",
      "Epoch: 125 \tTraining Loss: 0.325199 \tValidation Loss: 0.278009\n",
      "Epoch: 126 \tTraining Loss: 0.321271 \tValidation Loss: 0.288027\n",
      "Epoch: 127 \tTraining Loss: 0.314990 \tValidation Loss: 0.265850\n",
      "Epoch: 128 \tTraining Loss: 0.313032 \tValidation Loss: 0.293766\n",
      "Epoch: 129 \tTraining Loss: 0.326654 \tValidation Loss: 0.267162\n",
      "Epoch: 130 \tTraining Loss: 0.322090 \tValidation Loss: 0.283502\n",
      "Epoch: 131 \tTraining Loss: 0.321226 \tValidation Loss: 0.261609\n",
      "Epoch: 132 \tTraining Loss: 0.327907 \tValidation Loss: 0.268836\n",
      "Epoch: 133 \tTraining Loss: 0.312834 \tValidation Loss: 0.286828\n",
      "Epoch: 134 \tTraining Loss: 0.313472 \tValidation Loss: 0.285473\n",
      "Epoch: 135 \tTraining Loss: 0.320256 \tValidation Loss: 0.251231\n",
      "Validation loss decreased (0.253311 --> 0.251231. Saving model...\n",
      "Epoch: 136 \tTraining Loss: 0.313131 \tValidation Loss: 0.287653\n",
      "Epoch: 137 \tTraining Loss: 0.307193 \tValidation Loss: 0.255530\n",
      "Epoch: 138 \tTraining Loss: 0.307079 \tValidation Loss: 0.274799\n",
      "Epoch: 139 \tTraining Loss: 0.315485 \tValidation Loss: 0.274398\n",
      "Epoch: 140 \tTraining Loss: 0.319075 \tValidation Loss: 0.268056\n",
      "Epoch: 141 \tTraining Loss: 0.315671 \tValidation Loss: 0.310670\n",
      "Epoch: 142 \tTraining Loss: 0.310777 \tValidation Loss: 0.265545\n",
      "Epoch: 143 \tTraining Loss: 0.315636 \tValidation Loss: 0.275217\n",
      "Epoch: 144 \tTraining Loss: 0.325727 \tValidation Loss: 0.257754\n",
      "Epoch: 145 \tTraining Loss: 0.313187 \tValidation Loss: 0.271633\n",
      "Epoch: 146 \tTraining Loss: 0.319521 \tValidation Loss: 0.254240\n",
      "Epoch: 147 \tTraining Loss: 0.311847 \tValidation Loss: 0.296655\n",
      "Epoch: 148 \tTraining Loss: 0.312757 \tValidation Loss: 0.256213\n",
      "Epoch: 149 \tTraining Loss: 0.308026 \tValidation Loss: 0.245159\n",
      "Validation loss decreased (0.251231 --> 0.245159. Saving model...\n",
      "Epoch: 150 \tTraining Loss: 0.303661 \tValidation Loss: 0.251656\n",
      "Epoch: 151 \tTraining Loss: 0.311612 \tValidation Loss: 0.287305\n",
      "Epoch: 152 \tTraining Loss: 0.318515 \tValidation Loss: 0.269288\n",
      "Epoch: 153 \tTraining Loss: 0.311704 \tValidation Loss: 0.278163\n",
      "Epoch: 154 \tTraining Loss: 0.311192 \tValidation Loss: 0.265720\n",
      "Epoch: 155 \tTraining Loss: 0.309982 \tValidation Loss: 0.292611\n",
      "Epoch: 156 \tTraining Loss: 0.316936 \tValidation Loss: 0.280630\n",
      "Epoch: 157 \tTraining Loss: 0.306876 \tValidation Loss: 0.258752\n",
      "Epoch: 158 \tTraining Loss: 0.303056 \tValidation Loss: 0.266949\n",
      "Epoch: 159 \tTraining Loss: 0.316537 \tValidation Loss: 0.268297\n",
      "Epoch: 160 \tTraining Loss: 0.316624 \tValidation Loss: 0.264220\n",
      "Epoch: 161 \tTraining Loss: 0.310543 \tValidation Loss: 0.259166\n",
      "Epoch: 162 \tTraining Loss: 0.309187 \tValidation Loss: 0.242819\n",
      "Validation loss decreased (0.245159 --> 0.242819. Saving model...\n",
      "Epoch: 163 \tTraining Loss: 0.316024 \tValidation Loss: 0.256458\n",
      "Epoch: 164 \tTraining Loss: 0.304024 \tValidation Loss: 0.252018\n",
      "Epoch: 165 \tTraining Loss: 0.309926 \tValidation Loss: 0.261022\n",
      "Epoch: 166 \tTraining Loss: 0.302293 \tValidation Loss: 0.250302\n",
      "Epoch: 167 \tTraining Loss: 0.308926 \tValidation Loss: 0.247657\n",
      "Epoch: 168 \tTraining Loss: 0.314968 \tValidation Loss: 0.252060\n",
      "Epoch: 169 \tTraining Loss: 0.306566 \tValidation Loss: 0.278544\n",
      "Epoch: 170 \tTraining Loss: 0.307671 \tValidation Loss: 0.245926\n",
      "Epoch: 171 \tTraining Loss: 0.306387 \tValidation Loss: 0.255844\n",
      "Epoch: 172 \tTraining Loss: 0.302803 \tValidation Loss: 0.251969\n",
      "Epoch: 173 \tTraining Loss: 0.306952 \tValidation Loss: 0.268828\n",
      "Epoch: 174 \tTraining Loss: 0.318645 \tValidation Loss: 0.257358\n",
      "Epoch: 175 \tTraining Loss: 0.301058 \tValidation Loss: 0.244394\n",
      "Epoch: 176 \tTraining Loss: 0.306446 \tValidation Loss: 0.243729\n",
      "Epoch: 177 \tTraining Loss: 0.311623 \tValidation Loss: 0.261703\n",
      "Epoch: 178 \tTraining Loss: 0.307617 \tValidation Loss: 0.274681\n",
      "Epoch: 179 \tTraining Loss: 0.304781 \tValidation Loss: 0.256299\n",
      "Epoch: 180 \tTraining Loss: 0.300264 \tValidation Loss: 0.242789\n",
      "Validation loss decreased (0.242819 --> 0.242789. Saving model...\n",
      "Epoch: 181 \tTraining Loss: 0.298446 \tValidation Loss: 0.275035\n",
      "Epoch: 182 \tTraining Loss: 0.308405 \tValidation Loss: 0.299093\n",
      "Epoch: 183 \tTraining Loss: 0.305707 \tValidation Loss: 0.247684\n",
      "Epoch: 184 \tTraining Loss: 0.298051 \tValidation Loss: 0.304743\n",
      "Epoch: 185 \tTraining Loss: 0.309977 \tValidation Loss: 0.254711\n",
      "Epoch: 186 \tTraining Loss: 0.306659 \tValidation Loss: 0.244612\n",
      "Epoch: 187 \tTraining Loss: 0.305839 \tValidation Loss: 0.260280\n",
      "Epoch: 188 \tTraining Loss: 0.302370 \tValidation Loss: 0.277787\n",
      "Epoch: 189 \tTraining Loss: 0.302047 \tValidation Loss: 0.251623\n",
      "Epoch: 190 \tTraining Loss: 0.294744 \tValidation Loss: 0.254372\n",
      "Epoch: 191 \tTraining Loss: 0.307181 \tValidation Loss: 0.240899\n",
      "Validation loss decreased (0.242789 --> 0.240899. Saving model...\n",
      "Epoch: 192 \tTraining Loss: 0.303988 \tValidation Loss: 0.264124\n",
      "Epoch: 193 \tTraining Loss: 0.299490 \tValidation Loss: 0.262356\n",
      "Epoch: 194 \tTraining Loss: 0.299459 \tValidation Loss: 0.253559\n",
      "Epoch: 195 \tTraining Loss: 0.305981 \tValidation Loss: 0.236649\n",
      "Validation loss decreased (0.240899 --> 0.236649. Saving model...\n",
      "Epoch: 196 \tTraining Loss: 0.301364 \tValidation Loss: 0.243529\n",
      "Epoch: 197 \tTraining Loss: 0.300579 \tValidation Loss: 0.254203\n",
      "Epoch: 198 \tTraining Loss: 0.303034 \tValidation Loss: 0.247300\n",
      "Epoch: 199 \tTraining Loss: 0.304332 \tValidation Loss: 0.253978\n",
      "Epoch: 200 \tTraining Loss: 0.301207 \tValidation Loss: 0.234464\n",
      "Validation loss decreased (0.236649 --> 0.234464. Saving model...\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = make_data(df, target_df, KNNImputer(n_neighbors=5))\n",
    "train_loader, valid_loader, test_loader = create_loaders(train_data, test_data, batch_size=16, valid_size=0.2)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}\n",
    "model_KNN = train(200, loaders, model_KNN, get_optimizer(model_KNN), criterion, use_cuda, 'model_KNN.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.251692\n",
      "\n",
      "\n",
      "Test Accuracy: 90% (1545/1715)\n"
     ]
    }
   ],
   "source": [
    "model_KNN.load_state_dict(torch.load('model_KNN.pt'))\n",
    "test(loaders, model_KNN, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iter Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.520616 \tValidation Loss: 0.509352\n",
      "Validation loss decreased (inf --> 0.509352. Saving model...\n",
      "Epoch: 2 \tTraining Loss: 0.499205 \tValidation Loss: 0.493779\n",
      "Validation loss decreased (0.509352 --> 0.493779. Saving model...\n",
      "Epoch: 3 \tTraining Loss: 0.488492 \tValidation Loss: 0.499518\n",
      "Epoch: 4 \tTraining Loss: 0.463698 \tValidation Loss: 0.446626\n",
      "Validation loss decreased (0.493779 --> 0.446626. Saving model...\n",
      "Epoch: 5 \tTraining Loss: 0.442005 \tValidation Loss: 0.412078\n",
      "Validation loss decreased (0.446626 --> 0.412078. Saving model...\n",
      "Epoch: 6 \tTraining Loss: 0.420981 \tValidation Loss: 0.394889\n",
      "Validation loss decreased (0.412078 --> 0.394889. Saving model...\n",
      "Epoch: 7 \tTraining Loss: 0.416764 \tValidation Loss: 0.387688\n",
      "Validation loss decreased (0.394889 --> 0.387688. Saving model...\n",
      "Epoch: 8 \tTraining Loss: 0.407111 \tValidation Loss: 0.459397\n",
      "Epoch: 9 \tTraining Loss: 0.410634 \tValidation Loss: 0.446147\n",
      "Epoch: 10 \tTraining Loss: 0.405852 \tValidation Loss: 0.373589\n",
      "Validation loss decreased (0.387688 --> 0.373589. Saving model...\n",
      "Epoch: 11 \tTraining Loss: 0.396087 \tValidation Loss: 0.371457\n",
      "Validation loss decreased (0.373589 --> 0.371457. Saving model...\n",
      "Epoch: 12 \tTraining Loss: 0.403576 \tValidation Loss: 0.349057\n",
      "Validation loss decreased (0.371457 --> 0.349057. Saving model...\n",
      "Epoch: 13 \tTraining Loss: 0.371057 \tValidation Loss: 0.342499\n",
      "Validation loss decreased (0.349057 --> 0.342499. Saving model...\n",
      "Epoch: 14 \tTraining Loss: 0.369289 \tValidation Loss: 0.339093\n",
      "Validation loss decreased (0.342499 --> 0.339093. Saving model...\n",
      "Epoch: 15 \tTraining Loss: 0.370705 \tValidation Loss: 0.341867\n",
      "Epoch: 16 \tTraining Loss: 0.350924 \tValidation Loss: 0.340327\n",
      "Epoch: 17 \tTraining Loss: 0.351873 \tValidation Loss: 0.327685\n",
      "Validation loss decreased (0.339093 --> 0.327685. Saving model...\n",
      "Epoch: 18 \tTraining Loss: 0.350141 \tValidation Loss: 0.334593\n",
      "Epoch: 19 \tTraining Loss: 0.358812 \tValidation Loss: 0.334670\n",
      "Epoch: 20 \tTraining Loss: 0.347168 \tValidation Loss: 0.328265\n",
      "Epoch: 21 \tTraining Loss: 0.343236 \tValidation Loss: 0.348190\n",
      "Epoch: 22 \tTraining Loss: 0.350550 \tValidation Loss: 0.312785\n",
      "Validation loss decreased (0.327685 --> 0.312785. Saving model...\n",
      "Epoch: 23 \tTraining Loss: 0.339929 \tValidation Loss: 0.311200\n",
      "Validation loss decreased (0.312785 --> 0.311200. Saving model...\n",
      "Epoch: 24 \tTraining Loss: 0.335100 \tValidation Loss: 0.324692\n",
      "Epoch: 25 \tTraining Loss: 0.337352 \tValidation Loss: 0.306356\n",
      "Validation loss decreased (0.311200 --> 0.306356. Saving model...\n",
      "Epoch: 26 \tTraining Loss: 0.331073 \tValidation Loss: 0.313584\n",
      "Epoch: 27 \tTraining Loss: 0.339901 \tValidation Loss: 0.325402\n",
      "Epoch: 28 \tTraining Loss: 0.327120 \tValidation Loss: 0.313026\n",
      "Epoch: 29 \tTraining Loss: 0.329624 \tValidation Loss: 0.326141\n",
      "Epoch: 30 \tTraining Loss: 0.326493 \tValidation Loss: 0.333013\n",
      "Epoch: 31 \tTraining Loss: 0.329423 \tValidation Loss: 0.306668\n",
      "Epoch: 32 \tTraining Loss: 0.326058 \tValidation Loss: 0.312952\n",
      "Epoch: 33 \tTraining Loss: 0.323584 \tValidation Loss: 0.313623\n",
      "Epoch: 34 \tTraining Loss: 0.328751 \tValidation Loss: 0.306636\n",
      "Epoch: 35 \tTraining Loss: 0.322753 \tValidation Loss: 0.297814\n",
      "Validation loss decreased (0.306356 --> 0.297814. Saving model...\n",
      "Epoch: 36 \tTraining Loss: 0.328970 \tValidation Loss: 0.295765\n",
      "Validation loss decreased (0.297814 --> 0.295765. Saving model...\n",
      "Epoch: 37 \tTraining Loss: 0.323173 \tValidation Loss: 0.305733\n",
      "Epoch: 38 \tTraining Loss: 0.313199 \tValidation Loss: 0.296098\n",
      "Epoch: 39 \tTraining Loss: 0.316500 \tValidation Loss: 0.288918\n",
      "Validation loss decreased (0.295765 --> 0.288918. Saving model...\n",
      "Epoch: 40 \tTraining Loss: 0.307993 \tValidation Loss: 0.288568\n",
      "Validation loss decreased (0.288918 --> 0.288568. Saving model...\n",
      "Epoch: 41 \tTraining Loss: 0.309674 \tValidation Loss: 0.306047\n",
      "Epoch: 42 \tTraining Loss: 0.305889 \tValidation Loss: 0.303528\n",
      "Epoch: 43 \tTraining Loss: 0.310749 \tValidation Loss: 0.312602\n",
      "Epoch: 44 \tTraining Loss: 0.308682 \tValidation Loss: 0.283148\n",
      "Validation loss decreased (0.288568 --> 0.283148. Saving model...\n",
      "Epoch: 45 \tTraining Loss: 0.315552 \tValidation Loss: 0.282378\n",
      "Validation loss decreased (0.283148 --> 0.282378. Saving model...\n",
      "Epoch: 46 \tTraining Loss: 0.306094 \tValidation Loss: 0.290217\n",
      "Epoch: 47 \tTraining Loss: 0.306916 \tValidation Loss: 0.281130\n",
      "Validation loss decreased (0.282378 --> 0.281130. Saving model...\n",
      "Epoch: 48 \tTraining Loss: 0.295967 \tValidation Loss: 0.289235\n",
      "Epoch: 49 \tTraining Loss: 0.301921 \tValidation Loss: 0.274503\n",
      "Validation loss decreased (0.281130 --> 0.274503. Saving model...\n",
      "Epoch: 50 \tTraining Loss: 0.303699 \tValidation Loss: 0.309173\n",
      "Epoch: 51 \tTraining Loss: 0.297976 \tValidation Loss: 0.281109\n",
      "Epoch: 52 \tTraining Loss: 0.308325 \tValidation Loss: 0.302762\n",
      "Epoch: 53 \tTraining Loss: 0.301771 \tValidation Loss: 0.283102\n",
      "Epoch: 54 \tTraining Loss: 0.304136 \tValidation Loss: 0.272484\n",
      "Validation loss decreased (0.274503 --> 0.272484. Saving model...\n",
      "Epoch: 55 \tTraining Loss: 0.310717 \tValidation Loss: 0.284916\n",
      "Epoch: 56 \tTraining Loss: 0.294450 \tValidation Loss: 0.265917\n",
      "Validation loss decreased (0.272484 --> 0.265917. Saving model...\n",
      "Epoch: 57 \tTraining Loss: 0.302955 \tValidation Loss: 0.267865\n",
      "Epoch: 58 \tTraining Loss: 0.300066 \tValidation Loss: 0.279095\n",
      "Epoch: 59 \tTraining Loss: 0.298806 \tValidation Loss: 0.282847\n",
      "Epoch: 60 \tTraining Loss: 0.298287 \tValidation Loss: 0.282514\n",
      "Epoch: 61 \tTraining Loss: 0.298460 \tValidation Loss: 0.266039\n",
      "Epoch: 62 \tTraining Loss: 0.304724 \tValidation Loss: 0.269147\n",
      "Epoch: 63 \tTraining Loss: 0.287819 \tValidation Loss: 0.285278\n",
      "Epoch: 64 \tTraining Loss: 0.289580 \tValidation Loss: 0.277686\n",
      "Epoch: 65 \tTraining Loss: 0.287679 \tValidation Loss: 0.287988\n",
      "Epoch: 66 \tTraining Loss: 0.291622 \tValidation Loss: 0.266669\n",
      "Epoch: 67 \tTraining Loss: 0.293352 \tValidation Loss: 0.272638\n",
      "Epoch: 68 \tTraining Loss: 0.283846 \tValidation Loss: 0.274681\n",
      "Epoch: 69 \tTraining Loss: 0.291643 \tValidation Loss: 0.286805\n",
      "Epoch: 70 \tTraining Loss: 0.285664 \tValidation Loss: 0.282833\n",
      "Epoch: 71 \tTraining Loss: 0.282308 \tValidation Loss: 0.270178\n",
      "Epoch: 72 \tTraining Loss: 0.281022 \tValidation Loss: 0.313530\n",
      "Epoch: 73 \tTraining Loss: 0.283465 \tValidation Loss: 0.264359\n",
      "Validation loss decreased (0.265917 --> 0.264359. Saving model...\n",
      "Epoch: 74 \tTraining Loss: 0.288151 \tValidation Loss: 0.276792\n",
      "Epoch: 75 \tTraining Loss: 0.283007 \tValidation Loss: 0.267304\n",
      "Epoch: 76 \tTraining Loss: 0.282093 \tValidation Loss: 0.263119\n",
      "Validation loss decreased (0.264359 --> 0.263119. Saving model...\n",
      "Epoch: 77 \tTraining Loss: 0.280382 \tValidation Loss: 0.288581\n",
      "Epoch: 78 \tTraining Loss: 0.286862 \tValidation Loss: 0.255818\n",
      "Validation loss decreased (0.263119 --> 0.255818. Saving model...\n",
      "Epoch: 79 \tTraining Loss: 0.282128 \tValidation Loss: 0.251617\n",
      "Validation loss decreased (0.255818 --> 0.251617. Saving model...\n",
      "Epoch: 80 \tTraining Loss: 0.273783 \tValidation Loss: 0.271403\n",
      "Epoch: 81 \tTraining Loss: 0.269827 \tValidation Loss: 0.276816\n",
      "Epoch: 82 \tTraining Loss: 0.269472 \tValidation Loss: 0.258078\n",
      "Epoch: 83 \tTraining Loss: 0.267682 \tValidation Loss: 0.236864\n",
      "Validation loss decreased (0.251617 --> 0.236864. Saving model...\n",
      "Epoch: 84 \tTraining Loss: 0.274784 \tValidation Loss: 0.245121\n",
      "Epoch: 85 \tTraining Loss: 0.268245 \tValidation Loss: 0.237896\n",
      "Epoch: 86 \tTraining Loss: 0.261512 \tValidation Loss: 0.231474\n",
      "Validation loss decreased (0.236864 --> 0.231474. Saving model...\n",
      "Epoch: 87 \tTraining Loss: 0.265371 \tValidation Loss: 0.243845\n",
      "Epoch: 88 \tTraining Loss: 0.262389 \tValidation Loss: 0.236494\n",
      "Epoch: 89 \tTraining Loss: 0.266917 \tValidation Loss: 0.237856\n",
      "Epoch: 90 \tTraining Loss: 0.269284 \tValidation Loss: 0.243461\n",
      "Epoch: 91 \tTraining Loss: 0.265172 \tValidation Loss: 0.225184\n",
      "Validation loss decreased (0.231474 --> 0.225184. Saving model...\n",
      "Epoch: 92 \tTraining Loss: 0.259262 \tValidation Loss: 0.228701\n",
      "Epoch: 93 \tTraining Loss: 0.263476 \tValidation Loss: 0.227056\n",
      "Epoch: 94 \tTraining Loss: 0.266983 \tValidation Loss: 0.228329\n",
      "Epoch: 95 \tTraining Loss: 0.258344 \tValidation Loss: 0.232050\n",
      "Epoch: 96 \tTraining Loss: 0.258367 \tValidation Loss: 0.237507\n",
      "Epoch: 97 \tTraining Loss: 0.255870 \tValidation Loss: 0.234698\n",
      "Epoch: 98 \tTraining Loss: 0.259726 \tValidation Loss: 0.224475\n",
      "Validation loss decreased (0.225184 --> 0.224475. Saving model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 99 \tTraining Loss: 0.257543 \tValidation Loss: 0.221669\n",
      "Validation loss decreased (0.224475 --> 0.221669. Saving model...\n",
      "Epoch: 100 \tTraining Loss: 0.254532 \tValidation Loss: 0.227360\n",
      "Epoch: 101 \tTraining Loss: 0.257024 \tValidation Loss: 0.234554\n",
      "Epoch: 102 \tTraining Loss: 0.261890 \tValidation Loss: 0.233743\n",
      "Epoch: 103 \tTraining Loss: 0.249801 \tValidation Loss: 0.225708\n",
      "Epoch: 104 \tTraining Loss: 0.249635 \tValidation Loss: 0.229011\n",
      "Epoch: 105 \tTraining Loss: 0.251282 \tValidation Loss: 0.217928\n",
      "Validation loss decreased (0.221669 --> 0.217928. Saving model...\n",
      "Epoch: 106 \tTraining Loss: 0.251573 \tValidation Loss: 0.221297\n",
      "Epoch: 107 \tTraining Loss: 0.251609 \tValidation Loss: 0.217237\n",
      "Validation loss decreased (0.217928 --> 0.217237. Saving model...\n",
      "Epoch: 108 \tTraining Loss: 0.256248 \tValidation Loss: 0.221468\n",
      "Epoch: 109 \tTraining Loss: 0.253262 \tValidation Loss: 0.248782\n",
      "Epoch: 110 \tTraining Loss: 0.245555 \tValidation Loss: 0.217241\n",
      "Epoch: 111 \tTraining Loss: 0.254129 \tValidation Loss: 0.239135\n",
      "Epoch: 112 \tTraining Loss: 0.244996 \tValidation Loss: 0.217235\n",
      "Validation loss decreased (0.217237 --> 0.217235. Saving model...\n",
      "Epoch: 113 \tTraining Loss: 0.239106 \tValidation Loss: 0.238269\n",
      "Epoch: 114 \tTraining Loss: 0.240689 \tValidation Loss: 0.209525\n",
      "Validation loss decreased (0.217235 --> 0.209525. Saving model...\n",
      "Epoch: 115 \tTraining Loss: 0.249117 \tValidation Loss: 0.220526\n",
      "Epoch: 116 \tTraining Loss: 0.236164 \tValidation Loss: 0.230055\n",
      "Epoch: 117 \tTraining Loss: 0.244081 \tValidation Loss: 0.204405\n",
      "Validation loss decreased (0.209525 --> 0.204405. Saving model...\n",
      "Epoch: 118 \tTraining Loss: 0.237929 \tValidation Loss: 0.210432\n",
      "Epoch: 119 \tTraining Loss: 0.243406 \tValidation Loss: 0.231126\n",
      "Epoch: 120 \tTraining Loss: 0.256000 \tValidation Loss: 0.210811\n",
      "Epoch: 121 \tTraining Loss: 0.242993 \tValidation Loss: 0.204328\n",
      "Validation loss decreased (0.204405 --> 0.204328. Saving model...\n",
      "Epoch: 122 \tTraining Loss: 0.240319 \tValidation Loss: 0.207094\n",
      "Epoch: 123 \tTraining Loss: 0.253869 \tValidation Loss: 0.220276\n",
      "Epoch: 124 \tTraining Loss: 0.250519 \tValidation Loss: 0.207920\n",
      "Epoch: 125 \tTraining Loss: 0.240046 \tValidation Loss: 0.211036\n",
      "Epoch: 126 \tTraining Loss: 0.247678 \tValidation Loss: 0.216746\n",
      "Epoch: 127 \tTraining Loss: 0.236903 \tValidation Loss: 0.204348\n",
      "Epoch: 128 \tTraining Loss: 0.235804 \tValidation Loss: 0.207261\n",
      "Epoch: 129 \tTraining Loss: 0.237614 \tValidation Loss: 0.234246\n",
      "Epoch: 130 \tTraining Loss: 0.239696 \tValidation Loss: 0.216100\n",
      "Epoch: 131 \tTraining Loss: 0.232622 \tValidation Loss: 0.189666\n",
      "Validation loss decreased (0.204328 --> 0.189666. Saving model...\n",
      "Epoch: 132 \tTraining Loss: 0.241695 \tValidation Loss: 0.212428\n",
      "Epoch: 133 \tTraining Loss: 0.237122 \tValidation Loss: 0.234635\n",
      "Epoch: 134 \tTraining Loss: 0.237031 \tValidation Loss: 0.216168\n",
      "Epoch: 135 \tTraining Loss: 0.232408 \tValidation Loss: 0.202965\n",
      "Epoch: 136 \tTraining Loss: 0.238683 \tValidation Loss: 0.217940\n",
      "Epoch: 137 \tTraining Loss: 0.240565 \tValidation Loss: 0.213401\n",
      "Epoch: 138 \tTraining Loss: 0.234965 \tValidation Loss: 0.200736\n",
      "Epoch: 139 \tTraining Loss: 0.227055 \tValidation Loss: 0.186581\n",
      "Validation loss decreased (0.189666 --> 0.186581. Saving model...\n",
      "Epoch: 140 \tTraining Loss: 0.233365 \tValidation Loss: 0.190611\n",
      "Epoch: 141 \tTraining Loss: 0.222532 \tValidation Loss: 0.196580\n",
      "Epoch: 142 \tTraining Loss: 0.226702 \tValidation Loss: 0.206019\n",
      "Epoch: 143 \tTraining Loss: 0.239733 \tValidation Loss: 0.242629\n",
      "Epoch: 144 \tTraining Loss: 0.235671 \tValidation Loss: 0.183408\n",
      "Validation loss decreased (0.186581 --> 0.183408. Saving model...\n",
      "Epoch: 145 \tTraining Loss: 0.238307 \tValidation Loss: 0.201870\n",
      "Epoch: 146 \tTraining Loss: 0.228665 \tValidation Loss: 0.185674\n",
      "Epoch: 147 \tTraining Loss: 0.223833 \tValidation Loss: 0.200949\n",
      "Epoch: 148 \tTraining Loss: 0.231522 \tValidation Loss: 0.213302\n",
      "Epoch: 149 \tTraining Loss: 0.225118 \tValidation Loss: 0.196387\n",
      "Epoch: 150 \tTraining Loss: 0.222496 \tValidation Loss: 0.204269\n",
      "Epoch: 151 \tTraining Loss: 0.233302 \tValidation Loss: 0.185453\n",
      "Epoch: 152 \tTraining Loss: 0.226170 \tValidation Loss: 0.259382\n",
      "Epoch: 153 \tTraining Loss: 0.237107 \tValidation Loss: 0.176758\n",
      "Validation loss decreased (0.183408 --> 0.176758. Saving model...\n",
      "Epoch: 154 \tTraining Loss: 0.224988 \tValidation Loss: 0.201304\n",
      "Epoch: 155 \tTraining Loss: 0.220575 \tValidation Loss: 0.185372\n",
      "Epoch: 156 \tTraining Loss: 0.225681 \tValidation Loss: 0.197372\n",
      "Epoch: 157 \tTraining Loss: 0.218832 \tValidation Loss: 0.188598\n",
      "Epoch: 158 \tTraining Loss: 0.227183 \tValidation Loss: 0.241557\n",
      "Epoch: 159 \tTraining Loss: 0.224263 \tValidation Loss: 0.174591\n",
      "Validation loss decreased (0.176758 --> 0.174591. Saving model...\n",
      "Epoch: 160 \tTraining Loss: 0.231298 \tValidation Loss: 0.193161\n",
      "Epoch: 161 \tTraining Loss: 0.227702 \tValidation Loss: 0.216096\n",
      "Epoch: 162 \tTraining Loss: 0.224312 \tValidation Loss: 0.189103\n",
      "Epoch: 163 \tTraining Loss: 0.225621 \tValidation Loss: 0.194181\n",
      "Epoch: 164 \tTraining Loss: 0.226270 \tValidation Loss: 0.193454\n",
      "Epoch: 165 \tTraining Loss: 0.223152 \tValidation Loss: 0.182029\n",
      "Epoch: 166 \tTraining Loss: 0.217867 \tValidation Loss: 0.178740\n",
      "Epoch: 167 \tTraining Loss: 0.226327 \tValidation Loss: 0.186430\n",
      "Epoch: 168 \tTraining Loss: 0.220096 \tValidation Loss: 0.221551\n",
      "Epoch: 169 \tTraining Loss: 0.227842 \tValidation Loss: 0.197857\n",
      "Epoch: 170 \tTraining Loss: 0.225181 \tValidation Loss: 0.188821\n",
      "Epoch: 171 \tTraining Loss: 0.229169 \tValidation Loss: 0.189844\n",
      "Epoch: 172 \tTraining Loss: 0.218160 \tValidation Loss: 0.203887\n",
      "Epoch: 173 \tTraining Loss: 0.231328 \tValidation Loss: 0.188962\n",
      "Epoch: 174 \tTraining Loss: 0.216338 \tValidation Loss: 0.182827\n",
      "Epoch: 175 \tTraining Loss: 0.227580 \tValidation Loss: 0.184320\n",
      "Epoch: 176 \tTraining Loss: 0.218728 \tValidation Loss: 0.201648\n",
      "Epoch: 177 \tTraining Loss: 0.212497 \tValidation Loss: 0.176162\n",
      "Epoch: 178 \tTraining Loss: 0.217732 \tValidation Loss: 0.175704\n",
      "Epoch: 179 \tTraining Loss: 0.214523 \tValidation Loss: 0.171607\n",
      "Validation loss decreased (0.174591 --> 0.171607. Saving model...\n",
      "Epoch: 180 \tTraining Loss: 0.226399 \tValidation Loss: 0.183784\n",
      "Epoch: 181 \tTraining Loss: 0.213355 \tValidation Loss: 0.190140\n",
      "Epoch: 182 \tTraining Loss: 0.222358 \tValidation Loss: 0.172026\n",
      "Epoch: 183 \tTraining Loss: 0.223450 \tValidation Loss: 0.179772\n",
      "Epoch: 184 \tTraining Loss: 0.209428 \tValidation Loss: 0.178115\n",
      "Epoch: 185 \tTraining Loss: 0.222155 \tValidation Loss: 0.184966\n",
      "Epoch: 186 \tTraining Loss: 0.217060 \tValidation Loss: 0.210614\n",
      "Epoch: 187 \tTraining Loss: 0.225019 \tValidation Loss: 0.199791\n",
      "Epoch: 188 \tTraining Loss: 0.215536 \tValidation Loss: 0.186047\n",
      "Epoch: 189 \tTraining Loss: 0.222248 \tValidation Loss: 0.194813\n",
      "Epoch: 190 \tTraining Loss: 0.201938 \tValidation Loss: 0.181087\n",
      "Epoch: 191 \tTraining Loss: 0.211944 \tValidation Loss: 0.179958\n",
      "Epoch: 192 \tTraining Loss: 0.224374 \tValidation Loss: 0.179774\n",
      "Epoch: 193 \tTraining Loss: 0.214872 \tValidation Loss: 0.198257\n",
      "Epoch: 194 \tTraining Loss: 0.217038 \tValidation Loss: 0.175851\n",
      "Epoch: 195 \tTraining Loss: 0.200373 \tValidation Loss: 0.184751\n",
      "Epoch: 196 \tTraining Loss: 0.227061 \tValidation Loss: 0.192295\n",
      "Epoch: 197 \tTraining Loss: 0.207537 \tValidation Loss: 0.170167\n",
      "Validation loss decreased (0.171607 --> 0.170167. Saving model...\n",
      "Epoch: 198 \tTraining Loss: 0.205896 \tValidation Loss: 0.174966\n",
      "Epoch: 199 \tTraining Loss: 0.208850 \tValidation Loss: 0.186310\n",
      "Epoch: 200 \tTraining Loss: 0.213800 \tValidation Loss: 0.165367\n",
      "Validation loss decreased (0.170167 --> 0.165367. Saving model...\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = make_data(df, target_df, IterativeImputer())\n",
    "train_loader, valid_loader, test_loader = create_loaders(train_data, test_data, batch_size=16, valid_size=0.2)\n",
    "loaders = {'train': train_loader, 'valid': valid_loader, 'test': test_loader}\n",
    "model_iter = train(200, loaders, model_iter, get_optimizer(model_iter), criterion, use_cuda, 'model_iter.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.322165\n",
      "\n",
      "\n",
      "Test Accuracy: 85% (1469/1715)\n"
     ]
    }
   ],
   "source": [
    "model_iter.load_state_dict(torch.load('model_iter.pt'))\n",
    "test(loaders, model_iter, criterion, use_cuda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings\n",
    "Most models have similar accuracy on the imputed data. The median imputation takes the shortest and iterative imputation takes the longest.\n",
    "\n",
    "More data would greatly aid the first model as the accuracy from the first model was low compared to the last three models (since the first model only had 30 features to train on). Even though the last 3 models had more accuracy, it may still be better to utilize the first model since extensive data was imputed on the last 3 and may not hold up to more real world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "97cc609b13305c559618ec78a438abc56230b9381f827f22d070313b9a1f3777"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
